"""
Enhanced PCAP analyzer for web interface integration
"""

import os
import re
import base64
import hashlib
import tempfile
import json
import binascii
import string
from datetime import datetime
from typing import Any, Dict, List, Optional, Set, Tuple
from scapy.all import rdpcap, PcapReader, TCP, IP, UDP, Raw
import tempfile
import audioop
import struct
import io
import zlib

from utils.parsers import PacketParser
from utils.patterns import PatternMatcher
from ctf_analyzer import CTFAnalyzer, NetworkTrafficDecoder, EncodingDecoder, PatternExtractor
from ctf_flag_reconstruction import FlagReconstructionEngine, create_flag_reconstruction_engine
from workflow_orchestrator import WorkflowOrchestrator, WorkflowStep, create_network_ctf_workflow
from multi_agent_system import MultiAgentCoordinator, NetworkAnalysisAgent, CryptoAnalysisAgent, WebAnalysisAgent, BinaryAnalysisAgent, create_multi_agent_system

class WebPcapAnalyzer:
    """Enhanced PCAP analyzer optimized for web interface"""
    
    def __init__(self, logger=None, ai_agent=None, ctf_analyzer=None):
        self.parser = PacketParser()
        self.pattern_matcher = PatternMatcher()
        self.logger = logger
        self.ai_agent = ai_agent
        self.ctf_analyzer = ctf_analyzer or CTFAnalyzer()
        self.network_decoder = NetworkTrafficDecoder()
        self.encoding_decoder = EncodingDecoder()
        self.pattern_extractor = PatternExtractor()
        self.flag_reconstruction_engine = create_flag_reconstruction_engine()
        
        # Initialize workflow orchestrator
        self.workflow_orchestrator = WorkflowOrchestrator(logger)
        
        # Initialize multi-agent system
        self.multi_agent_coordinator, self.agents = create_multi_agent_system(logger)
        
        # Configure network agent with our decoders
        if 'network' in self.agents:
            self.agents['network'].network_decoder = self.network_decoder
        
        # Configure crypto agent with our decoders
        if 'crypto' in self.agents:
            self.agents['crypto'].encoding_decoder = self.encoding_decoder
        
        # File signatures for carving
        self.file_signatures = {
            # Standard images
            b'\x89PNG\r\n\x1a\n': {'ext': 'png', 'name': 'PNG Image'},
            b'\xff\xd8\xff': {'ext': 'jpg', 'name': 'JPEG Image'},
            b'GIF87a': {'ext': 'gif', 'name': 'GIF Image'},
            b'GIF89a': {'ext': 'gif', 'name': 'GIF Image'},
            b'RIFF': {'ext': 'webp', 'name': 'WebP Image'},
            b'BM': {'ext': 'bmp', 'name': 'Bitmap Image'},
            
            # Documents
            b'%PDF': {'ext': 'pdf', 'name': 'PDF Document'},
            b'\xd0\xcf\x11\xe0\xa1\xb1\x1a\xe1': {'ext': 'doc', 'name': 'Microsoft Word Document'},
            b'PK\x03\x04\x14\x00\x06\x00': {'ext': 'docx', 'name': 'Microsoft Word Document (XML)'},
            b'{\\rtf': {'ext': 'rtf', 'name': 'Rich Text Format'},
            
            # Archives
            b'PK\x03\x04': {'ext': 'zip', 'name': 'ZIP Archive'},
            b'PK\x05\x06': {'ext': 'zip', 'name': 'ZIP Archive'},
            b'PK\x07\x08': {'ext': 'zip', 'name': 'ZIP Archive'},
            b'\x1f\x8b\x08': {'ext': 'gz', 'name': 'GZIP Archive'},
            b'BZh': {'ext': 'bz2', 'name': 'BZIP2 Archive'},
            b'\x37\x7a\xbc\xaf\x27\x1c': {'ext': '7z', 'name': '7-Zip Archive'},
            b'\x52\x61\x72\x21\x1a\x07': {'ext': 'rar', 'name': 'RAR Archive'},
            b'7z\xbc\xaf\x27\x1c': {'ext': '7z', 'name': '7-Zip Archive'},
            
            # Executables
            b'\x7fELF': {'ext': 'elf', 'name': 'ELF Binary'},
            b'MZ': {'ext': 'exe', 'name': 'Windows Executable'},
            b'\x00\x00\x01\x00': {'ext': 'ico', 'name': 'Windows Icon'},
            
            # Media
            b'RIFF': {'ext': 'wav', 'name': 'WAV Audio'},
            b'ID3': {'ext': 'mp3', 'name': 'MP3 Audio'},
            b'\x00\x00\x00\x20ftyp': {'ext': 'mp4', 'name': 'MP4 Video'},
            b'\x00\x00\x00\x18ftyp': {'ext': 'mp4', 'name': 'MP4 Video'},
            b'\x00\x00\x00\x1cftyp': {'ext': 'mp4', 'name': 'MP4 Video'},
            b'RIFF....AVI': {'ext': 'avi', 'name': 'AVI Video'},
            b'FLV\x01': {'ext': 'flv', 'name': 'Flash Video'},
            b'\x1aE\xdf\xa3': {'ext': 'mkv', 'name': 'Matroska Video'},
            
            # Database files
            b'SQLite format 3\x00': {'ext': 'db', 'name': 'SQLite Database'},
            b'\x00\x01\x02\x03': {'ext': 'dbf', 'name': 'dBase Database'},
            
            # Certificate files
            b'-----BEGIN': {'ext': 'pem', 'name': 'PEM Certificate'},
            b'0\x82': {'ext': 'der', 'name': 'DER Certificate'},
            
            # CTF-specific signatures
            b'FLAG{': {'ext': 'txt', 'name': 'CTF Flag File'},
            b'CTF{': {'ext': 'txt', 'name': 'CTF Challenge File'},
            b'PICOCTF{': {'ext': 'txt', 'name': 'PicoCTF Challenge File'},
            b'HTB{': {'ext': 'txt', 'name': 'HackTheBox Flag File'},
            b'DUCTF{': {'ext': 'txt', 'name': 'DownUnderCTF Flag File'},
            b'TJCTF{': {'ext': 'txt', 'name': 'TJCTF Flag File'},
            b'CSAW{': {'ext': 'txt', 'name': 'CSAW Flag File'},
            b'UTCTF{': {'ext': 'txt', 'name': 'UTCTF Flag File'},
            b'TAMU{': {'ext': 'txt', 'name': 'TAMU Flag File'},
            b'RCTF{': {'ext': 'txt', 'name': 'RCTF Flag File'},
            b'0xGame{': {'ext': 'txt', 'name': '0xGame Flag File'},
            
            # Steganography containers
            b'\x89PNG\r\n\x1a\n': {'ext': 'png', 'name': 'PNG with Potential Steganography'},
            b'\xff\xd8\xff': {'ext': 'jpg', 'name': 'JPEG with Potential Steganography'},
        }
        
        self.results = {
            'total_packets': 0,
            'analyzed_packets': 0,
            'findings': [],
            'statistics': {},
            'analysis_time': None,
            'file_info': {},
            'ctf_findings': [],
            'hints': [],
            'suspicious_packets': [],
            'decoded_data': [],
            'extracted_patterns': [],
            'potential_flags': [],
            'workflow_steps': [],
            'agent_activities': [],
            'multi_agent_report': {},
            'reconstructed_streams': {},
            'flag_reassemblies': [],
            'encryption_attempts': [],
            # New features
            'extracted_files': [],
            'sessions': {},
            'exploit_suggestions': [],
            'timeline': [],
            'ai_analysis_results': {
                'enhanced_findings': [],
                'confidence_scores': {},
                'risk_levels': {},
                'analysis_metadata': {
                    'timestamp': '',
                    'ai_agent_version': '',
                    'analysis_duration': 0
                }
            },
            'correlation_graph': {'nodes': [], 'edges': []},
            'ai_hints': [],
            'file_carving_results': [],
            'malware_analysis': [],
            'protocol_sessions': {},
            'session_views': {},
            'voip_audio': [],
            'protocol_details': [],
            'replay_commands': [],
            'jwt_tokens': []
        }
    
    def _reconstruct_tcp_streams(self, packets):
        """Reconstruct TCP streams from packets (optimized for performance)"""
        from scapy.all import TCP, IP, Raw
        streams = {}
        
        # Limit processing for very large captures
        max_packets_for_streams = 10000
        packets_to_process = packets[:max_packets_for_streams] if len(packets) > max_packets_for_streams else packets
        
        for i, pkt in enumerate(packets_to_process):
            if not pkt.haslayer(TCP) or not pkt.haslayer(IP):
                continue
                
            ip = pkt[IP]
            tcp = pkt[TCP]
            
            # Use 4-tuple as stream key (handle both directions)
            key_fwd = (ip.src, tcp.sport, ip.dst, tcp.dport)
            key_rev = (ip.dst, tcp.dport, ip.src, tcp.sport)
            
            if key_fwd in streams:
                stream = streams[key_fwd]
            elif key_rev in streams:
                stream = streams[key_rev]
            else:
                stream = {
                    'packets': [],
                    'src_ip': ip.src,
                    'src_port': tcp.sport,
                    'dst_ip': ip.dst,
                    'dst_port': tcp.dport,
                    'protocol': 'TCP',
                    'data': b'',
                    'packet_indices': [],
                    'http_requests': [],
                    'http_responses': []
                }
                streams[key_fwd] = stream
            
            # Add packet to stream (limit data to prevent memory issues)
            stream['packets'].append(pkt)
            stream['packet_indices'].append(i)
            
            # Append payload if present (with size limit)
            if pkt.haslayer(Raw):
                payload = pkt[Raw].load
                # Limit stream data size to prevent memory issues
                if len(stream['data']) < 1024 * 1024:  # 1MB limit per stream
                    stream['data'] += payload
            
            # Limit number of streams to prevent memory issues
            if len(streams) > 1000:
                break
        
        # Try to extract HTTP messages from streams (with error handling)
        for stream in streams.values():
            try:
                if len(stream['data']) > 0:
                    text = stream['data'].decode('utf-8', errors='ignore')
                    # Limit text processing to prevent hanging
                    if len(text) > 10000:
                        text = text[:10000]
                    
                    # Split HTTP requests/responses
                    http_msgs = text.split('\r\n\r\n')
                    for msg in http_msgs[:10]:  # Limit messages per stream
                        if msg.startswith(('GET', 'POST', 'PUT', 'DELETE', 'HEAD', 'OPTIONS')):
                            stream['http_requests'].append(msg[:1000])  # Limit message size
                        elif msg.startswith('HTTP/'):
                            stream['http_responses'].append(msg[:1000])  # Limit message size
            except Exception:
                # Skip problematic streams to prevent hanging
                continue
        
        return streams
    
    def _track_packet_sequences(self, packets):
        """Track packet sequences for flow analysis"""
        sequences = {}
        try:
            from scapy.all import IP, TCP, UDP
            
            for i, packet in enumerate(packets):
                if packet.haslayer(IP):
                    ip = packet[IP]
                    
                    # Create sequence key based on connection
                    if packet.haslayer(TCP):
                        tcp = packet[TCP]
                        key = f"{ip.src}:{tcp.sport}->{ip.dst}:{tcp.dport}"
                        protocol = 'TCP'
                    elif packet.haslayer(UDP):
                        udp = packet[UDP]
                        key = f"{ip.src}:{udp.sport}->{ip.dst}:{udp.dport}"
                        protocol = 'UDP'
                    else:
                        key = f"{ip.src}->{ip.dst}"
                        protocol = 'IP'
                    
                    if key not in sequences:
                        sequences[key] = {
                            'packets': [],
                            'protocol': protocol,
                            'first_seen': i,
                            'last_seen': i,
                            'packet_count': 0
                        }
                    
                    sequences[key]['packets'].append(i)
                    sequences[key]['last_seen'] = i
                    sequences[key]['packet_count'] += 1
                    
                    # Limit to prevent memory issues with large captures
                    if len(sequences) > 1000:
                        break
                        
        except Exception as e:
            # Fallback: return empty sequences if there's an error
            pass
            
        return sequences
    
    def _detect_fragmentation_patterns(self, packets):
        """Detect fragmentation patterns that might contain distributed flags"""
        fragments = []
        try:
            from scapy.all import IP, Raw
            
            for i, packet in enumerate(packets):
                if packet.haslayer(IP):
                    ip = packet[IP]
                    
                    # Check for IP fragmentation
                    if ip.flags & 0x1 or ip.frag != 0:  # More fragments or fragment offset
                        fragments.append({
                            'packet_index': i,
                            'fragment_id': ip.id,
                            'fragment_offset': ip.frag,
                            'more_fragments': bool(ip.flags & 0x1),
                            'src': ip.src,
                            'dst': ip.dst,
                            'protocol': 'IP_FRAG'
                        })
                
                # Check for potential data fragments in payload
                if packet.haslayer(Raw):
                    raw_data = packet[Raw].load
                    try:
                        text_data = raw_data.decode('utf-8', errors='ignore')
                        
                        # Look for partial flag patterns
                        partial_patterns = [
                            r'flag\{[^}]*$',  # Incomplete flag at end
                            r'^[^{]*\}',      # Incomplete flag at start  
                            r'[A-Za-z0-9+/]{10,}={0,2}$',  # Potential base64 fragment
                            r'^[A-Za-z0-9+/]{10,}={0,2}',  # Base64 fragment at start
                        ]
                        
                        for pattern in partial_patterns:
                            if re.search(pattern, text_data):
                                fragments.append({
                                    'packet_index': i,
                                    'pattern': pattern,
                                    'data': text_data[:100],  # First 100 chars
                                    'protocol': 'DATA_FRAG',
                                    'confidence': 60
                                })
                                break  # Only one pattern per packet
                                
                    except UnicodeDecodeError:
                        pass
                
                # Limit fragments to prevent performance issues
                if len(fragments) > 500:
                    break
                    
        except Exception as e:
            # Fallback: return empty fragments if there's an error
            pass
            
        return fragments
    
    def _reassemble_fragments(self, fragments):
        """Attempt to reassemble fragmented flags"""
        reassembled = []
        try:
            # Group fragments by potential relationships
            ip_groups = {}
            data_groups = []
            
            for frag in fragments:
                if frag.get('protocol') == 'IP_FRAG':
                    frag_id = frag.get('fragment_id')
                    if frag_id not in ip_groups:
                        ip_groups[frag_id] = []
                    ip_groups[frag_id].append(frag)
                elif frag.get('protocol') == 'DATA_FRAG':
                    data_groups.append(frag)
            
            # Reassemble IP fragments
            for frag_id, frags in ip_groups.items():
                if len(frags) > 1:
                    # Sort by fragment offset
                    frags.sort(key=lambda x: x.get('fragment_offset', 0))
                    reassembled.append({
                        'type': 'ip_reassembly',
                        'fragment_id': frag_id,
                        'fragment_count': len(frags),
                        'packet_indices': [f.get('packet_index') for f in frags],
                        'confidence': 70
                    })
            
            # Look for potential data fragment patterns
            if len(data_groups) > 1:
                # Simple heuristic: look for fragments close in time/sequence
                for i in range(len(data_groups) - 1):
                    curr = data_groups[i]
                    next_frag = data_groups[i + 1]
                    
                    # If packets are close together, they might be related
                    if abs(curr.get('packet_index', 0) - next_frag.get('packet_index', 0)) < 10:
                        reassembled.append({
                            'type': 'data_reassembly',
                            'fragments': [curr, next_frag],
                            'packet_indices': [curr.get('packet_index'), next_frag.get('packet_index')],
                            'confidence': 50
                        })
                        
        except Exception as e:
            # Fallback: return empty reassembled list if there's an error
            pass
            
        return reassembled

    def _reassemble_flag_chunks(self, findings):
        """Reassemble flags across findings using bracket balancing and stream proximity."""
        import re
        start_tokens = ['flag{', 'CTF{', 'HTB{', 'DUCTF{', 'PICOCTF{', 'FLAG{']
        
        # Sort by stream and packet index
        def sort_key(f):
            return (f.get('stream_id') or '', f.get('packet_index', 0))
        sorted_findings = sorted(findings, key=sort_key)

        reassembled = []
        buffer = None  # {'text': str, 'indices': [], 'stream': str, 'chunks': []}
        open_count = 0

        def flush_if_complete():
            nonlocal buffer, open_count
            if buffer and open_count == 0 and any(buffer['text'].startswith(t) for t in start_tokens) and buffer['text'].endswith('}'):
                flag_text = buffer['text']
                if len(flag_text) >= 8 and ('{' in flag_text and '}'):
                    reassembled.append({
                        'reassembled_flag': flag_text,
                        'flag_chunks': list(buffer['chunks']),
                        'packet_indices': list(buffer['indices']),
                        'stream_id': buffer['stream']
                    })
                buffer = None
                open_count = 0

        for f in sorted_findings:
            text = str(f.get('data', ''))
            stream = f.get('stream_id')
            pkt_idx = f.get('packet_index', 0)

            # Start a new buffer if we see a start token
            if any(tok in text for tok in start_tokens):
                # Reset buffer if changing streams or too far apart
                if buffer and (buffer['stream'] != stream or (pkt_idx - buffer['indices'][-1]) > 50):
                    buffer = None
                    open_count = 0
                start_pos = min([pos for pos in [text.find(tok) for tok in start_tokens] if pos != -1])
                candidate = text[start_pos:]
                if not buffer:
                    buffer = {'text': '', 'indices': [], 'stream': stream, 'chunks': []}
                    open_count = 0
                buffer['text'] += candidate
                buffer['indices'].append(pkt_idx)
                buffer['chunks'].append(candidate)
                open_count += candidate.count('{') - candidate.count('}')
                flush_if_complete()
                continue

            # If buffering, append subsequent text in same stream within proximity
            if buffer and buffer['stream'] == stream and (pkt_idx - buffer['indices'][-1]) <= 50:
                if text:
                    buffer['text'] += text
                    buffer['indices'].append(pkt_idx)
                    buffer['chunks'].append(text)
                    open_count += text.count('{') - text.count('}')
                    flush_if_complete()
                continue

            # Single finding complete flag fallback
            m = re.search(r'(?i)(flag|ctf|htb|ductf|picoctf)\{[^}]+\}', text)
            if m:
                reassembled.append({
                    'reassembled_flag': m.group(0),
                    'flag_chunks': [m.group(0)],
                    'packet_indices': [pkt_idx],
                    'stream_id': stream
                })

        return reassembled

    def _attempt_decryption(self, findings, user_decrypt_key):
        """Try to decrypt detected blobs with user-supplied key/password"""
        import base64
        from Crypto.Cipher import AES
        attempts = []
        if not user_decrypt_key:
            return attempts
        for f in findings:
            data = f.get('data', '')
            # Try base64 decode + XOR
            try:
                decoded = base64.b64decode(data)
                # XOR with key
                key = user_decrypt_key.encode()
                xored = bytes([b ^ key[i % len(key)] for i, b in enumerate(decoded)])
                attempts.append({'method': 'Base64+XOR', 'input': data, 'output': xored.decode('utf-8', errors='ignore'), 'status': 'success', 'key': user_decrypt_key})
            except Exception:
                pass
            # Try AES decryption (CBC, 16-byte key)
            try:
                if len(user_decrypt_key) in (16, 24, 32):
                    cipher = AES.new(user_decrypt_key.encode(), AES.MODE_ECB)
                    decrypted = cipher.decrypt(base64.b64decode(data))
                    attempts.append({'method': 'AES-ECB', 'input': data, 'output': decrypted.decode('utf-8', errors='ignore'), 'status': 'success', 'key': user_decrypt_key})
            except Exception:
                pass
        return attempts
    
    def analyze_file(self, file_path: str, search_options: Dict[str, bool], 
                    custom_regex: Optional[str] = None, progress_callback=None, user_decrypt_key: str = None) -> Dict[str, Any]:
        """
        Analyze PCAP file with given search options
        
        Args:
            file_path: Path to PCAP file
            search_options: Dict with search flags (flags, credentials, tokens, etc.)
            custom_regex: Optional custom regex pattern
            progress_callback: Optional callback for progress updates
            user_decrypt_key: Optional key for decryption attempts
        
        Returns:
            Analysis results dictionary
        """
        # Initialize results dict if needed
        if not hasattr(self, 'results'):
            self.results = {
                'findings': [],
                'decoded_data': [],
                'jwt_tokens': [],
                'extracted_patterns': [],
                'potential_flags': [],
                'ctf_findings': []
            }

        start_time = datetime.now()
        
        # Get file info
        try:
            file_size = os.path.getsize(file_path)
            self.results['file_info'] = {
                'name': os.path.basename(file_path),
                'size': file_size,
                'size_mb': round(file_size / (1024 * 1024), 2)
            }
        except Exception as e:
            raise Exception(f"Failed to get file info: {str(e)}")
        
        # Read packets
        if progress_callback:
            progress_callback("Reading PCAP file...")
        
        try:
            packets = rdpcap(file_path)
            self.results['total_packets'] = len(packets)
            # Store packets for steganography analysis
            self.results['packets'] = packets
            if len(packets) == 0:
                # Return valid empty results if no packets
                self.results.update({
                    'analyzed_packets': 0,
                    'findings': [],
                    'analysis_time': {'duration': str(datetime.now() - start_time)}
                })
                return self.results
        except Exception as e:
            raise Exception(f"Failed to read PCAP file: {str(e)}")
        
        # Enhanced fragmentation detection with timeout protection
        if progress_callback:
            progress_callback("Detecting fragmented data...")
        
        try:
            # Track packet sequences with timeout
            packet_sequences = self._track_packet_sequences(packets)
            self.results['packet_sequences'] = packet_sequences
        except Exception as e:
            if progress_callback:
                progress_callback(f"Packet sequence tracking failed: {str(e)[:50]}...")
            self.results['packet_sequences'] = {}
        
        try:
            # Detect fragmentation patterns with timeout
            fragments = self._detect_fragmentation_patterns(packets)
            self.results['detected_fragments'] = fragments
        except Exception as e:
            if progress_callback:
                progress_callback(f"Fragment detection failed: {str(e)[:50]}...")
            self.results['detected_fragments'] = []
        
        try:
            # Reassemble fragments with timeout
            reassembled_flags = self._reassemble_fragments(fragments if 'fragments' in locals() else [])
            self.results['reassembled_flags'] = reassembled_flags
        except Exception as e:
            if progress_callback:
                progress_callback(f"Fragment reassembly failed: {str(e)[:50]}...")
            self.results['reassembled_flags'] = []
        
        try:
            # Reconstruct TCP streams with timeout
            if progress_callback:
                progress_callback("Reconstructing TCP streams...")
            self.results['reconstructed_streams'] = self._reconstruct_tcp_streams(packets)
        except Exception as e:
            if progress_callback:
                progress_callback(f"TCP stream reconstruction failed: {str(e)[:50]}...")
            self.results['reconstructed_streams'] = {}
        
        # Determine search types
        search_types = []
        if search_options.get('flags', False):
            search_types.append('flag')
        if search_options.get('credentials', False):
            search_types.append('credentials')
        if search_options.get('tokens', False):
            search_types.append('tokens')
        if search_options.get('emails', False):
            search_types.append('emails')
        if search_options.get('hashes', False):
            search_types.append('hashes')
        
        # Analyze packets with performance optimizations
        analyzed_count = 0
        findings = []
        
        # Add packet limit to prevent hanging on very large captures
        max_packets = 50000  # Reasonable limit for analysis
        packets_to_analyze = packets[:max_packets] if len(packets) > max_packets else packets
        
        if len(packets) > max_packets:
            if progress_callback:
                progress_callback(f"Large capture detected ({len(packets)} packets). Analyzing first {max_packets} packets...")
        
        for i, packet in enumerate(packets_to_analyze):
            # More frequent progress updates for better user experience
            if progress_callback and i % 50 == 0:  # Update every 50 packets instead of 100
                progress = (i / len(packets_to_analyze)) * 100
                progress_callback(f"Analyzing packets... {progress:.1f}% ({i}/{len(packets_to_analyze)})")
            
            # Parse packet
            packet_data = self.parser.extract_data(packet)
            if not packet_data:
                continue
            
            analyzed_count += 1
            
            # Search for patterns
            packet_findings = self.pattern_matcher.search_patterns(
                packet_data, search_types, custom_regex
            )
            
            # Add timestamp and additional info to findings
            normalized_findings = []
            for finding in packet_findings:
                # Ensure finding is a dict
                if isinstance(finding, str):
                    finding = {'data': finding, 'type': 'raw', 'confidence': 80}
                elif not isinstance(finding, dict):
                    continue
                    
                # Create a new dict to avoid modifying the original
                normalized = {
                    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    'packet_index': i,
                    'src_ip': packet_data.get('src', ''),
                    'dst_ip': packet_data.get('dst', ''),
                    'data': finding.get('data', str(finding)),
                    'type': finding.get('type', 'unknown'),
                    'protocol': packet_data.get('protocol', 'Unknown'),
                    'confidence': finding.get('confidence', 85)  # Default high confidence
                }
                normalized.update(finding)
                
                # Categorize finding type for display
                if normalized.get('type') == 'flag':
                    normalized['display_type'] = 'FLAG'
                    normalized['icon'] = 'üö©'
                    normalized['confidence'] = finding.get('confidence', 95)  # Flags get highest confidence
                elif normalized.get('type') == 'credentials':
                    normalized['display_type'] = 'CREDENTIAL'
                    normalized['icon'] = 'üîê'
                    normalized['confidence'] = finding.get('confidence', 90)
                elif normalized.get('type') == 'tokens':
                    normalized['display_type'] = 'TOKEN'
                    normalized['icon'] = 'üé´'
                    normalized['confidence'] = finding.get('confidence', 88)
                elif normalized.get('type') == 'emails':
                    normalized['display_type'] = 'EMAIL'
                    normalized['icon'] = 'üìß'
                    normalized['confidence'] = finding.get('confidence', 85)
                elif normalized.get('type') == 'hashes':
                    normalized['display_type'] = 'HASH'
                    normalized['icon'] = 'üîí'
                    normalized['confidence'] = finding.get('confidence', 80)
                else:
                    normalized['display_type'] = 'CUSTOM'
                    normalized['icon'] = 'üîç'
                    normalized['confidence'] = finding.get('confidence', 75)
                
                normalized_findings.append(normalized)
            
            findings.extend(normalized_findings)
        
        self.results['analyzed_packets'] = analyzed_count
        self.results['findings'] = findings

        # Prepare packet data list for deeper CTF-oriented extraction/decoding
        packet_data_list = []
        for i, packet in enumerate(packets):
            pdata = self.parser.extract_data(packet)
            if pdata:
                pdata['packet_index'] = i
                packet_data_list.append(pdata)
        
        # Store packet data list for steganography analysis
        self.results['packet_data_list'] = packet_data_list

        # Always run decoding and pattern extraction (useful for CTFs)
        decoded_items = self._decode_potential_data(packet_data_list)
        if decoded_items:
            self.results['decoded_data'].extend(decoded_items)
            
        # Parse JWTs from raw and decoded content
        jwt_items = self._detect_and_parse_jwts(packet_data_list, decoded_items)
        if jwt_items:
            self.results['jwt_tokens'].extend(jwt_items)
            # Also surface as findings
            for j in jwt_items:
                self.results['findings'].append({
                    'type': 'jwt',
                    'display_type': 'JWT',
                    'icon': 'üé´',
                    'data': json.dumps(j.get('claims', {})),
                    'protocol': j.get('protocol', 'Unknown'),
                    'src_ip': j.get('src_ip', ''),
                    'dst_ip': j.get('dst_ip', ''),
                    'packet_index': j.get('packet_index'),
                    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                    })
        
        # Extract patterns from packets (MOVED OUT OF JWT BLOCK)
        extracted_patterns = self._extract_patterns_from_packets(packet_data_list)
        if extracted_patterns:
            self.results['extracted_patterns'].extend(extracted_patterns)
        
        # Identify potential flags (MOVED OUT OF JWT BLOCK)
        potential_flags, custom_flag_findings = self._identify_potential_flags(packet_data_list, custom_regex)
        
        # Create comprehensive findings list that includes ALL sources
        findings = []
        
        # 1. Add standard pattern-matched flags
        if 'potential_flags' in self.results:
            for flag in self.results['potential_flags']:
                # Ensure proper structure for UI display
                finding = {
                    'type': flag.get('type', 'direct_match'),
                    'display_type': 'FLAG' if 'flag{' in (flag.get('data') or '').lower() else 'POTENTIAL FLAG',
                    'icon': 'üèÜ' if 'flag{' in (flag.get('data') or '').lower() else 'üö©',
                    'data': flag.get('data', ''),
                    'packet_index': flag.get('packet_index', -1),
                    'context': flag.get('context', ''),
                    'confidence': flag.get('confidence', 90)
                }
                findings.append(finding)
        
        # 2. Add AI agent findings (CRITICAL FIX)
        agent_results = {}
        if self.ai_agent:
            agent_results = self.ai_agent.analyze(packet_data_list)
        for task_id, result in agent_results.items():
            if 'result' in result and isinstance(result['result'], dict):
                # Add AI-generated potential flags
                if 'potential_flags' in result['result']:
                    for flag in result['result']['potential_flags']:
                        # Normalize to standard finding structure
                        findings.append({
                            'type': 'ai_find',
                            'display_type': 'AI FINDING',
                            'icon': 'ü§ñ',
                            'data': flag.get('data', flag.get('flag', '')),
                            'packet_index': flag.get('packet_index', -1),
                            'context': flag.get('explanation', '')[:200],
                            'ai_explanation': flag.get('explanation', ''),
                            'confidence': flag.get('confidence', 85),
                            'source_agent': flag.get('source_agent', result.get('agent_id', 'ai'))
                        })
        
        # 3. Add crypto analysis findings (CRITICAL FIX)
        if 'decoded_data' in self.results:
            for decoded in self.results['decoded_data']:
                # Only add as findings if it looks like a flag
                if self._is_potential_flag(decoded.get('result', '')):
                    findings.append({
                        'type': 'crypto_flag',
                        'display_type': 'DECRYPTED FLAG',
                        'icon': 'üîì',
                        'data': decoded['result'],
                        'packet_index': decoded.get('packet_index', -1),
                        'decoding_chain': decoded.get('chain', []),
                        'confidence': decoded.get('confidence', 95),
                        'original_encoding': decoded.get('original_type', '')
                    })
        
        # 4. Add any custom flag findings from plugins
        if custom_flag_findings:
            findings.extend(custom_flag_findings)
        
        # Deduplicate findings based on content
        seen = set()
        unique_findings = []
        for finding in findings:
            finding_key = f"{finding.get('data', '')}|{finding.get('type', '')}"
            if finding_key not in seen and finding.get('data'):
                seen.add(finding_key)
                unique_findings.append(finding)
        
        # Final findings assignment
        self.results['findings'] = unique_findings
        
        # Attach stream context to findings
        for finding in unique_findings:
            # Find which stream this finding belongs to
            for stream_id, stream in self.results['reconstructed_streams'].items():
                if finding.get('packet_index') in stream['packet_indices']:
                    finding['stream_id'] = str(stream_id)
                    finding['stream_data'] = stream['data'][:1000].decode('utf-8', errors='ignore') if stream['data'] else ''
                    break
        
        # Generate statistics
        # Attach AI confidence scores to findings and generate final statistics
        try:
            enhanced_findings = self._enhance_findings_with_ai_analysis(unique_findings)
            if enhanced_findings:
                unique_findings = enhanced_findings
        except Exception as e:
            if self.logger:
                self.logger.error(f"AI analysis failed: {str(e)}")
        
        # CRITICAL: Integrate AI Analysis Enhanced Findings into main findings
        try:
            if self.ai_agent:
                # Get packet data as text for AI analysis
                packet_text = "\n".join([str(pdata.get('data', '')) for pdata in packet_data_list[:100]])[:10000]
                
                # Use the comprehensive AI analysis with all new features
                comprehensive_ai_result = self.ai_agent.comprehensive_ai_analysis(
                    packet_data=packet_text,
                    findings=unique_findings,
                    ctf_context=getattr(self, 'ctf_context', None),
                    user_question=None,  # Can be set if interactive mode is needed
                    conversation_history=None
                )
                
                if comprehensive_ai_result and isinstance(comprehensive_ai_result, dict):
                    # Store comprehensive AI results
                    self.results['comprehensive_ai_analysis'] = comprehensive_ai_result
                    
                    # Extract validated findings from false positive reduction
                    if comprehensive_ai_result.get('validated_findings'):
                        validated_findings = comprehensive_ai_result['validated_findings']
                        if isinstance(validated_findings, list):
                            # Replace original findings with validated ones
                            unique_findings = validated_findings
                    
                    # Add any reconstructed flags from multi-protocol analysis
                    if (comprehensive_ai_result.get('flag_reconstruction') and 
                        isinstance(comprehensive_ai_result['flag_reconstruction'], dict)):
                        reconstruction_result = comprehensive_ai_result['flag_reconstruction']
                        if 'reconstruction_analysis' in reconstruction_result:
                            # Parse reconstruction analysis for new flags
                            try:
                                import json
                                reconstruction_data = reconstruction_result['reconstruction_analysis']
                                if isinstance(reconstruction_data, str):
                                    # Try to extract JSON from the response
                                    import re
                                    json_match = re.search(r'\{.*\}', reconstruction_data, re.DOTALL)
                                    if json_match:
                                        parsed_reconstruction = json.loads(json_match.group())
                                        if 'reconstructed_flags' in parsed_reconstruction:
                                            for reconstructed_flag in parsed_reconstruction['reconstructed_flags']:
                                                if reconstructed_flag.get('flag_candidate'):
                                                    # Add reconstructed flag as a finding
                                                    flag_finding = {
                                                        'type': 'reconstructed_flag',
                                                        'display_type': 'RECONSTRUCTED FLAG',
                                                        'icon': 'üîóüèÜ',
                                                        'data': reconstructed_flag['flag_candidate'],
                                                        'packet_index': -1,
                                                        'context': f"Reconstructed via: {reconstructed_flag.get('reconstruction_method', 'Multi-protocol analysis')}",
                                                        'confidence': reconstructed_flag.get('confidence', 85),
                                                        'ai_analysis': {
                                                            'explanation': f"Flag reconstructed using {reconstructed_flag.get('reconstruction_method', 'advanced techniques')}",
                                                            'reconstruction_steps': reconstructed_flag.get('reconstruction_steps', []),
                                                            'validation_poc': reconstructed_flag.get('validation_poc', '')
                                                        }
                                                    }
                                                    unique_findings.append(flag_finding)
                            except Exception as e:
                                if self.logger:
                                    self.logger.warning(f"Could not parse flag reconstruction results: {e}")
                
                # Fallback to original AI analysis if comprehensive analysis fails
                else:
                    # Run standard AI analysis on current findings
                    ai_analysis_result = self.ai_agent.analyze_findings(unique_findings, packet_text)
                    
                    if (ai_analysis_result and isinstance(ai_analysis_result, dict) and 
                        'ai_analysis' in ai_analysis_result and 
                        ai_analysis_result['ai_analysis'].get('enhanced_findings')):
                        
                        # Extract AI enhanced findings
                        ai_enhanced_findings = ai_analysis_result['ai_analysis']['enhanced_findings']
                        
                        if isinstance(ai_enhanced_findings, list):
                            # Convert AI findings to standard finding format and add to main findings
                            for ai_finding in ai_enhanced_findings:
                                if isinstance(ai_finding, dict) and ai_finding.get('finding'):
                                    # Create standard finding from AI analysis
                                    standard_finding = {
                                        'type': 'ai_enhanced',
                                        'display_type': 'AI ENHANCED FLAG' if 'flag' in ai_finding.get('finding', '').lower() else 'AI FINDING',
                                        'icon': 'ü§ñüèÜ' if 'flag' in ai_finding.get('finding', '').lower() else 'ü§ñ',
                                        'data': ai_finding.get('finding', ''),
                                        'packet_index': -1,
                                        'context': ai_finding.get('reasoning', 'AI-discovered pattern'),
                                        'confidence': ai_finding.get('confidence', 75),
                                        'ai_analysis': {
                                            'explanation': ai_finding.get('explanation', 'AI-enhanced analysis'),
                                            'suggestions': ai_finding.get('suggestions', []),
                                            'confidence': ai_finding.get('confidence', 75)
                                        }
                                    }
                                    unique_findings.append(standard_finding)
                    
                    # Store the full AI analysis result
                    self.results['ai_analysis'] = ai_analysis_result
                
        except Exception as e:
            if self.logger:
                self.logger.error(f"AI analysis integration failed: {str(e)}")
                
        self.results['statistics'] = self._generate_statistics(unique_findings)
        
        # Update the final findings list
        self.results['findings'] = unique_findings
        
        # Run CTF-specific analysis
        if progress_callback:
            progress_callback("Running CTF analysis...")
        
        # Extract HTTP packets for specialized analysis
        http_packets = [p for i, p in enumerate(packets) if self.parser.extract_data(p) and
                      self.parser.extract_data(p).get('protocol') == 'HTTP']
        
        # Always run basic CTF analysis, enhanced if CTF mode is enabled
        ctf_mode_enabled = search_options.get('ctf_mode', True)  # Default to True for compatibility
        
        # Create basic CTF analysis results from current findings
        ctf_analysis = {
            'flag_candidates': [],
            'metadata': {
                'primary_protocol': self._get_primary_protocol(findings),
                'total_findings': len(findings),
                'analysis_mode': 'enhanced' if ctf_mode_enabled else 'basic'
            }
        }
        
        # Extract flag candidates from findings
        for finding in findings:
            if finding.get('display_type') == 'FLAG' or 'flag' in str(finding.get('data', '')).lower():
                ctf_analysis['flag_candidates'].append({
                    'flag': finding.get('data', ''),
                    'confidence': finding.get('confidence', 80),
                    'pattern': finding.get('type', 'unknown'),
                    'protocol': finding.get('protocol', 'Unknown'),
                    'packet_number': finding.get('packet_index', 0) + 1,
                    'ai_analysis': f"Flag detected in {finding.get('protocol', 'unknown')} traffic with {finding.get('confidence', 80)}% confidence"
                })
        
        # Add potential flag candidates from decoded data
        for decoded in self.results.get('decoded_data', []):
            decoded_text = decoded.get('decoded', '') or decoded.get('result', '')
            if self._is_potential_flag(decoded_text):
                ctf_analysis['flag_candidates'].append({
                    'flag': decoded_text,
                    'confidence': 85,
                    'pattern': 'decoded_flag',
                    'protocol': decoded.get('protocol', 'Unknown'),
                    'packet_number': decoded.get('packet_index', 0) + 1,
                    'ai_analysis': f"Flag found in decoded {decoded.get('type', 'unknown')} data"
                })
        
        # Store CTF analysis results
        self.results['ctf_analysis'] = ctf_analysis
        
        # Perform specialized CTF analysis if mode is enabled
        if ctf_mode_enabled:
            if progress_callback:
                progress_callback("Setting up multi-agent analysis...")
            
            # Safely configure workflow orchestrator for network CTF challenges
            try:
                if hasattr(self, 'workflow_orchestrator') and self.workflow_orchestrator:
                    create_network_ctf_workflow(
                        self.workflow_orchestrator, 
                        self.parser, 
                        self.pattern_matcher, 
                        self.network_decoder, 
                        self.encoding_decoder, 
                        self.ctf_analyzer
                    )
                    
                    # Start the workflow with initial context
                    if progress_callback:
                        progress_callback("Starting multi-step analysis workflow...")
                    
                    workflow_result = self.workflow_orchestrator.start_workflow('network_ctf', {
                        'pcap_file': file_path,
                        'packet_data_list': packet_data_list
                    })
                    
                    # Execute all possible steps in the workflow
                    workflow_result = self.workflow_orchestrator.execute_all_steps()
                    
                    # Store workflow steps in results
                    self.results['workflow_steps'] = self.workflow_orchestrator.get_workflow_status().get('steps_history', [])
            except Exception as workflow_error:
                if self.logger:
                    self.logger.warning(f"Workflow orchestrator failed: {str(workflow_error)}")
                # Continue without workflow - not critical for basic analysis
            
            # Dispatch tasks to specialized agents
            try:
                if hasattr(self, 'multi_agent_coordinator') and self.multi_agent_coordinator:
                    if progress_callback:
                        progress_callback("Dispatching tasks to specialized agents...")
                    
                    # Network analysis agent
                    if hasattr(self, 'agents') and 'network' in self.agents:
                        self.multi_agent_coordinator.dispatch_task(
                            self.agents['network'].agent_id,
                            {
                                'type': 'analyze_network',
                                'packet_data': packet_data_list
                            }
                        )
                
                # Extract potential encoded data for crypto agent
                encoded_data = []
                for packet in packet_data_list:
                    if 'data' in packet and packet['data']:
                        # Look for potential base64 encoded strings
                        import re
                        base64_patterns = re.findall(r'[A-Za-z0-9+/]{40,}={0,2}', packet['data'])
                        encoded_data.extend(base64_patterns)
            
                # Web analysis agent - extract HTTP content
                html_content = []
                javascript_content = []
                http_requests = []
                
                for packet in packet_data_list:
                    if packet.get('protocol') == 'HTTP':
                        if 'http_type' in packet:
                            if packet['http_type'] == 'request':
                                http_requests.append(packet)
                            elif packet['http_type'] == 'response' and 'http_body' in packet:
                                if 'content_type' in packet and 'html' in packet['content_type'].lower():
                                    html_content.append(packet['http_body'])
                                elif 'content_type' in packet and 'javascript' in packet['content_type'].lower():
                                    javascript_content.append(packet['http_body'])
                
                # Web analysis agent
                if html_content or javascript_content or http_requests:
                    if hasattr(self, 'agents') and 'web' in self.agents:
                        self.multi_agent_coordinator.dispatch_task(
                            self.agents['web'].agent_id,
                            {
                                'type': 'analyze_web',
                                'html_content': '\n'.join(html_content) if html_content else None,
                                'javascript_content': '\n'.join(javascript_content) if javascript_content else None,
                                'http_requests': http_requests
                            }
                        )
                
                # Process agent tasks
                if progress_callback:
                    progress_callback("Processing agent tasks...")
                
                agent_results = self.multi_agent_coordinator.process_agent_tasks()
                
                # Process agent messages
                self.multi_agent_coordinator.process_messages()
                
                # Generate multi-agent report
                self.results['multi_agent_report'] = self.multi_agent_coordinator.generate_report()
                
                # Extract findings from agent results
                for task_id, result in agent_results.items():
                    agent_id = result['agent_id']
                    agent_result = result['result']
                    
                    # Extract potential flags
                    if 'potential_flags' in agent_result:
                        self.results['potential_flags'].extend(agent_result['potential_flags'])
                    
                    # Extract decoded data
                    if 'decoded_data' in agent_result:
                        self.results['decoded_data'].extend(agent_result['decoded_data'])
                    
                    # Extract HTTP findings
                    if 'http_findings' in agent_result:
                        self.results['ctf_findings'].extend(self._format_http_findings(agent_result['http_findings']))
                    
                    # Extract HTML analysis
                    if 'html_analysis' in agent_result and agent_result['html_analysis'].get('potential_flags'):
                        for flag in agent_result['html_analysis']['potential_flags']:
                            self.results['potential_flags'].append({
                                'data': flag,
                                'source': 'html_content',
                                'type': 'direct_match'
                            })
                
                # Store agent activities
                if hasattr(self, 'agents') and self.agents:
                    self.results['agent_activities'] = [
                        {
                            'agent_id': agent_id,
                            'name': agent.name,
                            'tasks_completed': sum(1 for task in agent.tasks if task['status'] == 'completed'),
                            'tasks_failed': sum(1 for task in agent.tasks if task['status'] == 'failed')
                        } for agent_id, agent in self.agents.items()
                    ]
                
                # Generate hints based on findings
                if search_options.get('ctf_mode', False):
                    try:
                        self.results['hints'] = self._generate_hints()
                    except Exception:
                        self.results['hints'] = ["üîç Analysis hints temporarily unavailable"]
                else:
                    # Basic hints for non-CTF mode
                    self.results['hints'] = [
                        "üîç Try looking for base64 encoded data in unusual places",
                        "üåê Check DNS queries for hidden data or tunneling",
                        "‚è∞ Analyze packet timing patterns for steganography"
                    ]
            except Exception as agent_error:
                if self.logger:
                    self.logger.warning(f"Multi-agent analysis failed: {str(agent_error)}")
                # Continue without multi-agent analysis - not critical for basic analysis
        
        # Identify suspicious packets
        try:
            self.results['suspicious_packets'] = self._identify_suspicious_packets(packet_data_list)
        except Exception:
            self.results['suspicious_packets'] = []
        
        # After findings are generated:
        try:
            self.results['flag_reassemblies'] = self._reassemble_flag_chunks(findings)
            # Attach reassembly context to findings
            for reassembly in self.results['flag_reassemblies']:
                for f in findings:
                    if f['packet_index'] in reassembly['packet_indices']:
                        f['flag_chunks'] = reassembly['flag_chunks']
                        f['reassembled_flag'] = reassembly['reassembled_flag']
        except Exception:
            self.results['flag_reassemblies'] = []
        
        # Enhanced Flag Reconstruction Engine
        try:
            self.results['flag_reconstruction'] = self.flag_reconstruction_engine.reconstruct_distributed_flags(findings)
            
            # Add reconstructed flags to findings with enhanced metadata
            for reconstructed in self.results['flag_reconstruction'].get('reconstructed_flags', []):
                findings.append({
                    'type': 'reconstructed_flag',
                    'display_type': 'RECONSTRUCTED FLAG',
                    'data': reconstructed['flag'],
                    'confidence': int(reconstructed['confidence'] * 100),
                    'reconstruction_method': reconstructed.get('reconstruction_method', 'advanced_engine'),
                    'source_fragments': len(reconstructed.get('fragments', [])),
                    'packet_indices': reconstructed.get('packet_indices', []),
                    'protocols': reconstructed.get('protocols', []),
                    'source': 'enhanced_flag_reconstruction_engine',
                    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                })
        except Exception as e:
            self.results['flag_reconstruction'] = {'error': str(e), 'reconstructed_flags': []}
        
        # Analysis time
        end_time = datetime.now()
        self.results['analysis_time'] = {
            'duration': str(end_time - start_time),
            'start': start_time.isoformat(),
            'end': end_time.isoformat()
        }
        
        if progress_callback:
            progress_callback("Analysis complete!")
        
        # Generate exploit suggestions
        self.results['exploit_suggestions'] = self._generate_exploit_suggestions(findings)
        
        # Extract files from TCP/UDP streams using file signatures
        try:
            self.results['extracted_files'] = self._carve_files_from_streams(self.results['reconstructed_streams'])
            # Extract HTTP transferred files
            http_files = self._extract_http_files(packets)
            self.results['extracted_files'].extend(http_files)
            # Analyze carved files for metadata and stego indicators
            if self.results['extracted_files']:
                self.results['extracted_files'] = self._analyze_carved_files(self.results['extracted_files'])
        except Exception as e:
            if self.logger:
                self.logger.warning(f"File carving failed: {str(e)}")
            self.results['extracted_files'] = []
        
        # Build timeline first
        try:
            self.results['timeline'] = self._build_timeline(packets, findings)
            self.results['ai_hints'] = self._generate_ai_hints(findings, self.results['timeline'], challenge_description=None)
        except Exception:
            self.results['timeline'] = []
            self.results['ai_hints'] = []

        # Build sessions, then session views
        try:
            self.results['sessions'] = self._build_sessions(packets)
            self.results['session_views'] = self._reconstruct_sessions(packets, self.results['sessions'])
        except Exception:
            self.results['sessions'] = {}
            self.results['session_views'] = {}

        # Protocol details aggregation
        try:
            self.results['protocol_details'] = self._collect_protocol_details(findings, self.results['sessions'])
        except Exception:
            self.results['protocol_details'] = {}

        # Correlation graph (after sessions are available)
        try:
            self.results['correlation_graph'] = self._build_correlation_graph(
                findings,
                self.results['reconstructed_streams'],
                self.results['sessions']
            )
        except Exception:
            self.results['correlation_graph'] = {}

        # VoIP audio extraction
        try:
            self.results['voip_audio'] = self._extract_voip_audio(self.results['sessions'])
        except Exception:
            self.results['voip_audio'] = []
        
        # Replay commands
        try:
            self.results['replay_commands'] = self._build_replay_commands(self.results['sessions'])
        except Exception:
            self.results['replay_commands'] = []
        
        return self.results
    
    def _get_primary_protocol(self, findings):
        """Determine the primary protocol from findings"""
        if not findings:
            return 'Unknown'
        
        protocol_counts = {}
        for finding in findings:
            protocol = finding.get('protocol', 'Unknown')
            protocol_counts[protocol] = protocol_counts.get(protocol, 0) + 1
        
        # Return the most common protocol
        return max(protocol_counts.items(), key=lambda x: x[1])[0] if protocol_counts else 'Unknown'
        
    def _generate_exploit_suggestions(self, findings: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Generate high-level, safe suggestions based on findings.
        
        This implementation is intentionally conservative and avoids providing
        actual exploit details. It returns investigation suggestions that help
        guide analysis without revealing sensitive details.
        """
        suggestions = []
        
        try:
            # Track what we've seen to avoid duplicate suggestions
            seen_types = set()
            
            for f in (findings or []):
                f_type = f.get('display_type', '').upper()
                if f_type in seen_types:
                    continue
                seen_types.add(f_type)
                
                # JWT token guidance
                if 'JWT' in f_type or (f.get('data') and 'eyJ' in str(f.get('data'))):
                    suggestions.append({
                        'type': 'jwt_analysis',
                        'title': 'JWT Token Analysis',
                        'finding_type': f_type,
                        'risk_level': 'Medium',
                        'suggestion': 'Review JWT claims and headers. Consider checking signature verification and expiration.',
                        'packet_index': f.get('packet_index'),
                        'protocol': f.get('protocol', 'Unknown')
                    })
                
                # Suspicious HTTP patterns
                if f.get('protocol') == 'HTTP' and any(x in str(f.get('data', '')).upper() for x in ['UNION', 'SELECT', 'INSERT', 'DROP']):
                    suggestions.append({
                        'type': 'http_injection',
                        'title': 'HTTP Parameter Analysis',
                        'finding_type': f_type,
                        'risk_level': 'High',
                        'suggestion': 'Review HTTP parameters for potential SQL patterns. Consider input validation.',
                        'packet_index': f.get('packet_index'),
                        'protocol': 'HTTP'
                    })
                
                # Base64/encoded content
                if 'BASE64' in f_type or (f.get('data') and re.search(r'[A-Za-z0-9+/]{30,}={0,2}', str(f.get('data')))):
                    suggestions.append({
                        'type': 'encoded_content',
                        'title': 'Encoded Content Analysis',
                        'finding_type': f_type,
                        'risk_level': 'Low',
                        'suggestion': 'Review base64-encoded content. Consider multi-layer decoding.',
                        'packet_index': f.get('packet_index'),
                        'protocol': f.get('protocol', 'Unknown')
                    })
                
                # Flag format detection
                if 'FLAG' in f_type or self._is_potential_flag(str(f.get('data', ''))):
                    suggestions.append({
                        'type': 'flag_analysis',
                        'title': 'Potential Flag Analysis',
                        'finding_type': f_type,
                        'risk_level': 'Info',
                        'suggestion': 'Verify flag format and consider surrounding packet context.',
                        'packet_index': f.get('packet_index'),
                        'protocol': f.get('protocol', 'Unknown')
                    })
                
                # DNS anomalies
                if f.get('protocol') == 'DNS' and len(str(f.get('data', ''))) > 200:
                    suggestions.append({
                        'type': 'dns_analysis',
                        'title': 'DNS Query Analysis',
                        'finding_type': f_type,
                        'risk_level': 'Medium',
                        'suggestion': 'Review long DNS queries for potential DNS tunneling.',
                        'packet_index': f.get('packet_index'),
                        'protocol': 'DNS'
                    })
                
                # General binary/hex content
                if re.search(r'\\x[0-9a-fA-F]{2}', str(f.get('data', ''))):
                    suggestions.append({
                        'type': 'binary_analysis',
                        'title': 'Binary Content Review',
                        'finding_type': f_type,
                        'risk_level': 'Medium',
                        'suggestion': 'Examine hex-encoded content for potential shellcode or embedded files.',
                        'packet_index': f.get('packet_index'),
                        'protocol': f.get('protocol', 'Unknown')
                    })
            
            # Add protocol-specific suggestions based on what we've seen
            protocols = {f.get('protocol', '').upper() for f in findings if f.get('protocol')}
            
            if 'HTTP' in protocols:
                suggestions.append({
                    'type': 'http_review',
                    'title': 'HTTP Traffic Review',
                    'finding_type': 'PROTOCOL',
                    'risk_level': 'Info',
                    'suggestion': 'Review HTTP headers, cookies, and POST data for sensitive information.',
                    'protocol': 'HTTP'
                })
            
            if 'DNS' in protocols:
                suggestions.append({
                    'type': 'dns_review',
                    'title': 'DNS Query Review',
                    'finding_type': 'PROTOCOL',
                    'risk_level': 'Info',
                    'suggestion': 'Analyze DNS query patterns and subdomain structure.',
                    'protocol': 'DNS'
                })
            
            # Deduplicate by suggestion type
            seen = set()
            unique = []
            for s in suggestions:
                key = f"{s['type']}|{s.get('packet_index', '')}"
                if key not in seen:
                    seen.add(key)
                    unique.append(s)
            
            return unique
        
        except Exception:
            # On any error, return an empty list to keep pipeline running
            return []
    
    def _detect_packet_obfuscation(self, packet_data_list: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Detect common packet obfuscation techniques"""
        obfuscation_findings = []
        
        # 1. Check for fragmented packets that could hide data
        fragmented_packets = [
            p for p in packet_data_list 
            if p.get('flags') == 'MF' or (p.get('flags') == 'DF' and p.get('fragment_offset', 0) > 0)
        ]
        if fragmented_packets:
            primary_packet = fragmented_packets[0]
            evidence_packets = [p['packet_index'] for p in fragmented_packets]
            obfuscation_findings.append({
                'technique': 'Packet Fragmentation',
                'confidence': 85,
                'evidence': f"{len(fragmented_packets)} fragmented packets detected",
                'packet_indices': evidence_packets,
                'primary_packet_index': primary_packet['packet_index'],
                'evidence_snippet': f"Fragment offset: {primary_packet.get('fragment_offset', 'N/A')}"
            })
        
        # 2. Check for unusual IP ID values (sequential/random)
        ip_ids = [p['ip_id'] for p in packet_data_list if 'ip_id' in p]
        if ip_ids and len(set(ip_ids)) < len(ip_ids) * 0.3:  # High repetition
            obfuscation_findings.append({
                'technique': 'IP ID Spoofing',
                'confidence': 70,
                'evidence': f"Highly repetitive IP IDs ({len(ip_ids)-len(set(ip_ids))} duplicates)",
                'packet_indices': [p['packet_index'] for p in packet_data_list if 'ip_id' in p and ip_ids.count(p['ip_id']) > 1],
                'primary_packet_index': packet_data_list[0]['packet_index'],
                'evidence_snippet': f"Common IP IDs: {', '.join(str(x) for x in list(set(ip_ids))[:3])}"
            })
        
        # 3. Check for oversized DNS requests (tunneling)
        dns_packets = [p for p in packet_data_list if p.get('protocol') == 'DNS' and len(p.get('data', '')) > 250]
        if dns_packets:
            primary_packet = dns_packets[0]
            obfuscation_findings.append({
                'technique': 'DNS Tunneling',
                'confidence': 90,
                'evidence': f"{len(dns_packets)} oversized DNS packets detected (>250 bytes)",
                'packet_indices': [p['packet_index'] for p in dns_packets],
                'primary_packet_index': primary_packet['packet_index'],
                'evidence_snippet': f"DNS size: {len(primary_packet['data'])} bytes"
            })
        
        # 4. Check for HTTP request smuggling patterns
        http_packets = [p for p in packet_data_list if p.get('protocol') == 'HTTP']
        smuggling_patterns = []
        for packet in http_packets:
            headers = packet.get('http_headers', '')
            if 'Content-Length' in headers and 'Transfer-Encoding' in headers:
                smuggling_patterns.append(packet)
            if 'chunked' in headers and 'Content-Length' in headers:
                smuggling_patterns.append(packet)
                
        if smuggling_patterns:
            primary_packet = smuggling_patterns[0]
            obfuscation_findings.append({
                'technique': 'HTTP Request Smuggling',
                'confidence': 95,
                'evidence': f"{len(smuggling_patterns)} potential HTTP smuggling vectors detected",
                'packet_indices': [p['packet_index'] for p in smuggling_patterns],
                'primary_packet_index': primary_packet['packet_index'],
                'evidence_snippet': primary_packet.get('http_headers', '')[:100]
            })
        
        # 5. Check for unusual TCP flag combinations
        suspicious_tcp_flags = []
        for packet in packet_data_list:
            if packet.get('protocol') == 'TCP':
                flags = packet.get('tcp_flags', '')
                # SYN+FIN is unusual
                if 'S' in flags and 'F' in flags:
                    suspicious_tcp_flags.append(packet)
        
        if suspicious_tcp_flags:
            primary_packet = suspicious_tcp_flags[0]
            obfuscation_findings.append({
                'technique': 'Suspicious TCP Flags',
                'confidence': 75,
                'evidence': f"{len(suspicious_tcp_flags)} packets with unusual TCP flag combinations",
                'packet_indices': [p['packet_index'] for p in suspicious_tcp_flags],
                'primary_packet_index': primary_packet['packet_index'],
                'evidence_snippet': f"Flags: {primary_packet.get('tcp_flags', 'N/A')}"
            })
        
        # 6. Check for data encoded in packet timing
        # (Simplified version - would need actual timing analysis)
        if len(packet_data_list) > 10:
            time_deltas = []  # Would normally calculate packet time differentials
            timing_pattern = False
            if timing_pattern:  # Would have actual pattern recognition logic
                obfuscation_findings.append({
                    'technique': 'Timing-Based Covert Channel',
                    'confidence': 80,
                    'evidence': "Suspicious packet timing patterns detected",
                    'packet_indices': [p['packet_index'] for p in packet_data_list],
                    'primary_packet_index': packet_data_list[0]['packet_index'],
                    'evidence_snippet': "Irregular packet intervals detected"
                })
        
        # 7. Check for data hidden in padding
        padded_packets = [
            p for p in packet_data_list 
            if p.get('padding_size', 0) > 10 and len(p.get('data', '')) > p.get('padding_size', 0)
        ]
        if padded_packets:
            primary_packet = padded_packets[0]
            obfuscation_findings.append({
                'technique': 'Padding-Based Hiding',
                'confidence': 65,
                'evidence': f"{len(padded_packets)} packets with significant padding detected",
                'packet_indices': [p['packet_index'] for p in padded_packets],
                'primary_packet_index': primary_packet['packet_index'],
                'evidence_snippet': f"Padding size: {primary_packet.get('padding_size', 'N/A')} bytes"
            })
        
        # 8. Check for protocol tunneling (e.g., HTTP over DNS)
        protocol_mismatches = []
        for packet in packet_data_list:
            if packet.get('protocol') == 'DNS' and 'HTTP' in (packet.get('data', '')[:200]):
                protocol_mismatches.append(packet)
            if packet.get('protocol') == 'ICMP' and len(packet.get('data', '')) > 100:
                protocol_mismatches.append(packet)
                
        if protocol_mismatches:
            primary_packet = protocol_mismatches[0]
            obfuscation_findings.append({
                'technique': 'Protocol Tunneling',
                'confidence': 85,
                'evidence': f"{len(protocol_mismatches)} potential protocol tunneling instances",
                'packet_indices': [p['packet_index'] for p in protocol_mismatches],
                'primary_packet_index': primary_packet['packet_index'],
                'evidence_snippet': f"Mismatch: {primary_packet.get('protocol')} containing other protocol data"
            })
        
        return obfuscation_findings

    def _carve_files_from_streams(self, streams: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Scan reconstructed streams for known file signatures and return carved file metadata.

        This is a conservative, non-invasive implementation intended to provide
        useful metadata to the rest of the pipeline. It does not attempt deep
        carving or full file recovery ‚Äî it only locates signature offsets and
        extracts a bounded slice around each hit.
        """
        carved = []
        if not streams:
            return carved

        # For each stream, search for file signatures in the byte blob
        for stream_id, stream in (streams.items() if isinstance(streams, dict) else []):
            try:
                data = stream.get('data') if isinstance(stream, dict) else None
                if not data or not isinstance(data, (bytes, bytearray)):
                    continue

                for sig, meta in self.file_signatures.items():
                    start = 0
                    while True:
                        idx = data.find(sig, start)
                        if idx == -1:
                            break
                        # Extract with improved boundary detection
                        fragment = self._extract_data_with_boundaries(data, idx, meta)
                        if fragment:
                            filename = f"carved_{stream_id}_{idx}.{meta.get('ext','bin')}"
                            carved.append({
                                'stream_id': stream_id,
                                'offset': idx,
                                'size': len(fragment),
                                'name': filename,
                                'ext': meta.get('ext', 'bin'),
                                'description': meta.get('name', 'unknown'),
                                'data': fragment
                            })
                        start = idx + max(1, len(sig))
            except Exception:
                # Be conservative: if anything goes wrong, skip this stream
                continue

        return carved

    def _extract_data_with_boundaries(self, data: bytes, pos: int, file_info: Dict[str, str]) -> Optional[bytes]:
        """Extract data with proper boundary detection for various file formats"""
        try:
            ext = file_info.get('ext', 'bin')
            name = file_info.get('name', 'unknown')
            
            # Handle special cases for different file types
            if ext in ['png', 'jpg', 'gif']:
                if ext == 'png':
                    end_marker = b'\x00\x00\x00\x00IEND\xaeB`\x82'
                elif ext == 'jpg':
                    end_marker = b'\xff\xd9'
                else:  # gif
                    end_marker = b'\x00\x3b'
                
                # Look for end marker with reasonable search limit
                search_limit = min(pos + 50 * 1024 * 1024, len(data))  # 50MB limit
                end_pos = data.find(end_marker, pos, search_limit)
                if end_pos != -1:
                    # Include the end marker in the extracted data
                    marker_length = len(end_marker)
                    return data[pos:end_pos + marker_length]
                else:
                    # If no end marker found, extract a reasonable chunk
                    chunk_size = min(10 * 1024 * 1024, len(data) - pos)  # 10MB or remaining data
                    return data[pos:pos + chunk_size]
            
            elif ext == 'pdf':
                # For PDF, look for %%EOF
                end_pos = data.find(b'%%EOF', pos)
                if end_pos != -1:
                    return data[pos:end_pos + 5]
                else:
                    # Extract a reasonable chunk
                    chunk_size = min(5 * 1024 * 1024, len(data) - pos)  # 5MB or remaining data
                    return data[pos:pos + chunk_size]
            
            elif ext in ['zip', 'doc']:
                # For ZIP and DOC files, extract a reasonable chunk
                chunk_size = min(20 * 1024 * 1024, len(data) - pos)  # 20MB or remaining data
                return data[pos:pos + chunk_size]
            
            elif ext in ['gz', 'bz2', '7z', 'rar']:
                # For compressed archives, extract a reasonable chunk
                chunk_size = min(10 * 1024 * 1024, len(data) - pos)  # 10MB or remaining data
                return data[pos:pos + chunk_size]
            
            elif ext in ['exe', 'elf']:
                # For executables, extract with size heuristics
                chunk_size = min(50 * 1024 * 1024, len(data) - pos)  # 50MB or remaining data
                return data[pos:pos + chunk_size]
            
            elif 'CTF' in name or 'Flag' in name:
                # For CTF flag files, look for closing brace
                end_pos = data.find(b'}', pos)
                if end_pos != -1:
                    return data[pos:end_pos + 1]
                else:
                    # Extract a reasonable chunk for partial flags
                    chunk_size = min(1024, len(data) - pos)
                    return data[pos:pos + chunk_size]
            
            else:
                # Default extraction for other file types
                chunk_size = min(2 * 1024 * 1024, len(data) - pos)  # 2MB or remaining data
                return data[pos:pos + chunk_size]
                
        except Exception:
            # Fallback to simple extraction
            chunk_size = min(1024 * 1024, len(data) - pos)  # 1MB or remaining data
            return data[pos:pos + chunk_size]

    def _build_timeline(self, packets: List[Any], findings: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Build a comprehensive timeline combining packet timestamps and finding timestamps.

        Returns a list of events sorted by timestamp. Each event is a dict with
        keys: 'datetime', 'type', 'protocol', 'src_ip', 'dst_ip', 'description', and optional 'packet_index'.
        This implementation provides rich timeline data for the UI.
        """
        events: List[Dict[str, Any]] = []
        try:
            # Add packet-based events with full details
            if packets:
                for i, pkt in enumerate(packets):
                    try:
                        ts = getattr(pkt, 'time', None)
                        if ts is not None:
                            # Extract packet details
                            src_ip = ""
                            dst_ip = ""
                            protocol = "Unknown"
                            src_port = ""
                            dst_port = ""
                            
                            # Try to get IP layer info
                            if hasattr(pkt, 'src'):
                                src_ip = pkt.src
                            if hasattr(pkt, 'dst'):
                                dst_ip = pkt.dst
                                
                            # Try scapy layers
                            try:
                                from scapy.layers.inet import IP, TCP, UDP
                                if pkt.haslayer(IP):
                                    ip_layer = pkt[IP]
                                    src_ip = ip_layer.src
                                    dst_ip = ip_layer.dst
                                    
                                    if pkt.haslayer(TCP):
                                        protocol = "TCP"
                                        src_port = str(pkt[TCP].sport)
                                        dst_port = str(pkt[TCP].dport)
                                    elif pkt.haslayer(UDP):
                                        protocol = "UDP"
                                        src_port = str(pkt[UDP].sport)
                                        dst_port = str(pkt[UDP].dport)
                                    else:
                                        protocol = "IP"
                            except Exception:
                                # Fallback to basic packet info
                                if hasattr(pkt, 'name'):
                                    protocol = pkt.name
                            
                            description = f"Packet {i}"
                            if src_ip and dst_ip:
                                description = f"{src_ip}"
                                if src_port:
                                    description += f":{src_port}"
                                description += f" -> {dst_ip}"
                                if dst_port:
                                    description += f":{dst_port}"
                                description += f" ({protocol})"
                            
                            events.append({
                                'timestamp': ts,
                                'datetime': datetime.fromtimestamp(ts).isoformat(),
                                'type': 'packet',
                                'protocol': protocol,
                                'src_ip': src_ip,
                                'dst_ip': dst_ip,
                                'src_port': src_port,
                                'dst_port': dst_port,
                                'description': description,
                                'packet_index': i
                            })
                    except Exception:
                        # Skip malformed packets
                        continue

            # Add finding events with full details
            for f in (findings or []):
                try:
                    t = None
                    if isinstance(f.get('timestamp'), str):
                        # try to parse ISO-like timestamp, fallback to None
                        try:
                            from datetime import datetime
                            t = datetime.fromisoformat(f['timestamp']).timestamp()
                        except Exception:
                            t = None
                    elif isinstance(f.get('timestamp'), (int, float)):
                        t = float(f.get('timestamp'))

                    if t is not None:
                        description = f.get('data', '')[:200] if f.get('data') else f.get('display_type', '')
                        if f.get('display_type'):
                            description = f"Found {f.get('display_type')}: {description}"
                        
                        events.append({
                            'timestamp': t,
                            'datetime': datetime.fromtimestamp(t).isoformat(),
                            'type': 'finding',
                            'protocol': f.get('protocol', ''),
                            'src_ip': f.get('src_ip', ''),
                            'dst_ip': f.get('dst_ip', ''),
                            'description': description,
                            'packet_index': f.get('packet_index'),
                            'finding_type': f.get('display_type', ''),
                            'data': f.get('data', '')
                        })
                except Exception:
                    # Skip malformed findings
                    continue

            # Sort events by time when available; stable sort keeps insertion order for None
            events = sorted(events, key=lambda e: (e.get('timestamp') is None, e.get('timestamp') if e.get('timestamp') is not None else 0))
        except Exception:
            # On any unexpected error, return an empty timeline to keep analysis robust
            return []

        return events

    def _build_sessions(self, packets: List[Any]) -> Dict[str, Any]:
        """Reconstruct simple session summaries from packets.

        Returns a dict mapping a session key to a small summary with packet indices,
        protocols and counts. This implementation is lightweight and defensive
        to avoid analysis crashes when packet structure is unexpected.
        """
        sessions: Dict[str, Any] = {}
        try:
            if not packets:
                return sessions

            for i, pkt in enumerate(packets):
                try:
                    # Prefer parser-extracted data if available
                    pdata = None
                    if hasattr(self, 'parser') and self.parser:
                        try:
                            pdata = self.parser.extract_data(pkt)
                        except Exception:
                            pdata = None

                    if pdata:
                        src = pdata.get('src') or pdata.get('src_ip') or ''
                        dst = pdata.get('dst') or pdata.get('dst_ip') or ''
                        sport = str(pdata.get('sport') or pdata.get('src_port') or '')
                        dport = str(pdata.get('dport') or pdata.get('dst_port') or '')
                        proto = pdata.get('protocol') or ''
                    else:
                        # Best-effort for scapy packet objects
                        src = getattr(pkt, 'src', '') or getattr(getattr(pkt, 'payload', None), 'src', '')
                        dst = getattr(pkt, 'dst', '') or getattr(getattr(pkt, 'payload', None), 'dst', '')
                        sport = str(getattr(pkt, 'sport', '') or '')
                        dport = str(getattr(pkt, 'dport', '') or '')
                        proto = getattr(pkt, 'proto', '') or getattr(pkt, 'name', '')

                    key = f"{src}:{sport}-{dst}:{dport}-{proto}"
                    if key not in sessions:
                        sessions[key] = {
                            'src': src,
                            'dst': dst,
                            'src_port': sport,
                            'dst_port': dport,
                            'protocol': proto,
                            'packet_indices': [],
                            'first_seen': i,
                            'last_seen': i,
                            'packet_count': 0
                        }

                    sessions[key]['packet_indices'].append(i)
                    sessions[key]['last_seen'] = i
                    sessions[key]['packet_count'] += 1
                    
                    # Extract FTP commands if this is an FTP session
                    if (dport == '21' or sport == '21' or proto.upper() == 'FTP') and hasattr(pkt, 'load'):
                        try:
                            payload = bytes(pkt.load).decode('utf-8', errors='ignore')
                            # Look for FTP commands
                            ftp_commands = ['USER', 'PASS', 'RETR', 'STOR', 'LIST', 'PASV', 'CWD', 'QUIT', 'DELE']
                            for line in payload.split('\n'):
                                line = line.strip()
                                for cmd in ftp_commands:
                                    if line.startswith(cmd + ' ') or line == cmd:
                                        if 'ftp_commands' not in sessions[key]:
                                            sessions[key]['ftp_commands'] = []
                                        
                                        # Extract command and arguments
                                        parts = line.split(' ', 1)
                                        command = parts[0]
                                        args = parts[1] if len(parts) > 1 else ''
                                        
                                        sessions[key]['ftp_commands'].append({
                                            'command': command,
                                            'args': args,
                                            'packet_number': i + 1,
                                            'full_line': line
                                        })
                                        
                                        # Add FTP credentials as findings
                                        if command in ['USER', 'PASS']:
                                            finding = {
                                                'type': 'ftp_credential',
                                                'display_type': 'CREDENTIAL',
                                                'icon': 'üîê',
                                                'data': f"FTP {command}: {args}",
                                                'packet_index': i,
                                                'src_ip': sip,
                                                'dst_ip': dip,
                                                'protocol': 'FTP',
                                                'confidence': 92,
                                                'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                                                'credential_type': 'username' if command == 'USER' else 'password',
                                                'service': 'FTP'
                                            }
                                            findings.append(finding)
                                        
                                        # Update protocol to FTP if not already set
                                        if not sessions[key]['protocol'] or sessions[key]['protocol'] == '':
                                            sessions[key]['protocol'] = 'FTP'
                                        break
                        except Exception:
                            pass  # Ignore parsing errors for individual packets
                except Exception:
                    # Skip malformed packet but continue
                    continue
        except Exception:
            return {}

        return sessions
    
    def _reconstruct_sessions(self, packets: List[Any], sessions: Dict[str, Any]) -> Dict[str, Any]:
        """Create a lightweight 'view' of sessions suitable for UI display.

        This function is defensive: it tolerates None/empty inputs and returns
        a dict mapping protocol types to session summaries. Each summary contains
        normalized messages suitable for UI display.
        """
        views: Dict[str, Dict[str, List[Dict[str, Any]]]] = {}
        try:
            if not sessions:
                return views

            for sid, s in sessions.items():
                try:
                    if not isinstance(s, dict):
                        continue
                        
                    protocol = s.get('protocol', 'UNKNOWN')
                    messages = []
                    
                    # Extract messages based on session type
                    if protocol == 'HTTP':
                        for req in s.get('http_requests', []):
                            messages.append({
                                'timestamp': s.get('timestamp', datetime.now().strftime('%Y-%m-%d %H:%M:%S')),
                                'direction': 'REQUEST',
                                'content': req,
                                'type': 'request'
                            })
                        for resp in s.get('http_responses', []):
                            messages.append({
                                'timestamp': s.get('timestamp', datetime.now().strftime('%Y-%m-%d %H:%M:%S')),
                                'direction': 'RESPONSE',
                                'content': resp,
                                'type': 'response'
                            })
                    else:
                        # Generic session data
                        content = s.get('data', '')
                        if content:
                            messages.append({
                                'timestamp': s.get('timestamp', datetime.now().strftime('%Y-%m-%d %H:%M:%S')),
                                'direction': f"{s.get('src_ip', 'unknown')} -> {s.get('dst_ip', 'unknown')}",
                                'content': content,
                                'type': 'data'
                            })
                            
                    if messages:
                        if protocol not in views:
                            views[protocol] = {}
                        views[protocol][sid] = messages
                except Exception:
                    # Skip problematic session entries but continue processing others
                    continue
        except Exception:
            return {}

        return views

    def _enhance_findings_with_ai_analysis(self, findings: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Enhance findings with AI-driven confidence scores and insights.
        Returns the enhanced findings list with AI analysis results.
        """
        if not findings:
            return []

        enhanced_findings = []
        
        try:
            for finding in findings:
                if not isinstance(finding, dict):
                    continue
                    
                # Create a deep copy to avoid modifying original
                enhanced = dict(finding)
                
                # Initialize AI analysis fields if not present
                if 'ai_analysis' not in enhanced:
                    enhanced['ai_analysis'] = {
                        'confidence': 0,
                        'explanation': '',
                        'suggestions': [],
                        'related_findings': [],
                        'risk_level': 'unknown'
                    }
                    
                # Perform AI analysis if agent is available
                if self.ai_agent and hasattr(self.ai_agent, 'analyze_finding'):
                    try:
                        ai_result = self.ai_agent.analyze_finding(enhanced)
                        if isinstance(ai_result, dict):
                            enhanced['ai_analysis'].update(ai_result)
                    except Exception as e:
                        # Log error but continue processing
                        if self.logger:
                            self.logger.error(f"AI analysis error: {str(e)}")
                
                enhanced_findings.append(enhanced)
                
        except Exception as e:
            if self.logger:
                self.logger.error(f"Error in AI analysis enhancement: {str(e)}")
            # Return original findings if enhancement fails
            return findings
            
        return enhanced_findings or findings

    def _generate_statistics(self, findings: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Generate analysis statistics from findings.

        Returns a dict with counts and distributions of finding types,
        protocols, confidence levels etc. This implementation is defensive
        and returns safe defaults on error.
        """
        stats: Dict[str, Any] = {
            'total_findings': 0,
            'by_type': {},
            'by_protocol': {},
            'by_confidence': {
                'high': 0,
                'medium': 0,
                'low': 0
            },
            'timestamp_range': {
                'first': None,
                'last': None
            }
        }

        try:
            stats['total_findings'] = len(findings)

            # Count by type
            for f in findings:
                # Display type counting
                disp_type = f.get('display_type', 'Unknown').upper()
                stats['by_type'][disp_type] = stats['by_type'].get(disp_type, 0) + 1

                # Protocol counting
                proto = (f.get('protocol') or 'Unknown').upper()
                stats['by_protocol'][proto] = stats['by_protocol'].get(proto, 0) + 1

                # Confidence bucketing
                conf = float(f.get('confidence', 0))
                if conf >= 0.8:
                    stats['by_confidence']['high'] += 1
                elif conf >= 0.5:
                    stats['by_confidence']['medium'] += 1
                else:
                    stats['by_confidence']['low'] += 1

                # Track timestamp range
                ts = f.get('timestamp')
                if isinstance(ts, str):
                    try:
                        dt = datetime.fromisoformat(ts.replace('Z', '+00:00'))
                        if not stats['timestamp_range']['first'] or dt < stats['timestamp_range']['first']:
                            stats['timestamp_range']['first'] = dt
                        if not stats['timestamp_range']['last'] or dt > stats['timestamp_range']['last']:
                            stats['timestamp_range']['last'] = dt
                    except Exception:
                        pass

            # Add ratio calculations if we have findings
            if stats['total_findings'] > 0:
                stats['ratios'] = {
                    'high_confidence': stats['by_confidence']['high'] / stats['total_findings'],
                    'flagged_suspicious': sum(1 for f in findings if 'SUSPIC' in (f.get('display_type') or '').upper()) / stats['total_findings']
                }

            # Convert datetime objects to isoformat strings for JSON serialization
            if stats['timestamp_range']['first']:
                stats['timestamp_range']['first'] = stats['timestamp_range']['first'].isoformat()
            if stats['timestamp_range']['last']:
                stats['timestamp_range']['last'] = stats['timestamp_range']['last'].isoformat()

        except Exception:
            # On any error, return minimal valid statistics
            stats = {
                'total_findings': len(findings),
                'by_type': {},
                'by_protocol': {},
                'by_confidence': {'high': 0, 'medium': 0, 'low': 0}
            }

        return stats

    def _generate_ai_hints(self, findings: List[Dict[str, Any]], timeline: List[Dict[str, Any]], challenge_description: Optional[str] = None) -> List[Dict[str, Any]]:
        """Generate safe, high-level AI-driven hints for the UI.

        This function is intentionally conservative: it doesn't provide exploit
        steps or sensitive instructions. It summarizes interesting signals
        and suggests safe investigation directions.
        """
        hints: List[Dict[str, Any]] = []
        try:
            if not findings:
                return hints

            # JWT hint
            if any('jwt' in (f.get('type') or '').lower() or (f.get('data') and 'eyJ' in str(f.get('data'))) for f in findings):
                hints.append({
                    'title': 'Inspect JWTs',
                    'hint': 'JWT-like tokens were detected. Review decoded claims for sensitive identifiers, expirations, and scope.',
                    'confidence': 90
                })

            # Flag/CTF hint
            if any('flag' in (f.get('type') or '').lower() or ('flag{' in (str(f.get('data') or '').lower())) for f in findings):
                hints.append({
                    'title': 'Potential Flags Found',
                    'hint': 'Content resembling CTF flags was detected. Verify reassembly across streams and decoding chains.',
                    'confidence': 95
                })

            # Suspicious packet hint
            if any((f.get('display_type') or '').upper().find('SUSPIC') != -1 for f in findings):
                hints.append({
                    'title': 'Review Suspicious Packets',
                    'hint': 'Packets flagged as suspicious may indicate tunneling or obfuscation. Consider timing and payload analyses.',
                    'confidence': 75
                })

            # Heuristic: many HTTP findings
            http_count = sum(1 for f in findings if (f.get('protocol') or '').upper() == 'HTTP' or (f.get('display_type') or '').upper().find('HTTP') != -1)
            if http_count > 5:
                hints.append({
                    'title': 'Web Surface Review',
                    'hint': 'Multiple HTTP-related findings detected. Check headers, cookies, and any exposed endpoints for sensitive data.',
                    'confidence': 80
                })

            # Generic fallback
            if not hints:
                hints.append({
                    'title': 'No High-Confidence AI Hints',
                    'hint': 'No clear patterns for AI hints. Consider running deeper decoders, multi-agent workflows, or manual review of unusual payloads.',
                    'confidence': 50
                })
        except Exception:
            return []

        return hints

    def _build_correlation_graph(self, findings: List[Dict[str, Any]], streams: Dict[str, Any], sessions: Dict[str, Any]) -> Dict[str, Any]:
        """Build a correlation graph connecting findings, streams, and sessions.
        
        Returns a dict with 'nodes' and 'edges' lists suitable for visualization.
        This implementation is defensive and lightweight to avoid analysis crashes.
        """
        graph = {
            'nodes': [],
            'edges': []
        }
        
        try:
            nodes = {}  # id -> node dict
            edges = set()  # (from_id, to_id, type) tuples
            
            # Helper to safely add nodes
            def add_node(id_: str, type_: str, label: str, data: Dict[str, Any] = None):
                if id_ not in nodes:
                    nodes[id_] = {
                        'id': id_,
                        'type': type_,
                        'label': label,
                        'data': data or {}
                    }
            
            # Helper to safely add edges
            def add_edge(from_id: str, to_id: str, type_: str):
                if from_id in nodes and to_id in nodes:
                    edges.add((from_id, to_id, type_))
            
            # Add findings as nodes
            for i, f in enumerate(findings or []):
                finding_id = f'finding_{i}'
                add_node(
                    finding_id,
                    'finding',
                    f"{f.get('display_type', 'Finding')}: {str(f.get('data', ''))[:30]}",
                    {
                        'type': f.get('type'),
                        'display_type': f.get('display_type'),
                        'protocol': f.get('protocol'),
                        'confidence': f.get('confidence', 50)
                    }
                )
                
                # Link to stream if available
                if f.get('stream_id'):
                    stream_id = f'stream_{f["stream_id"]}'
                    add_node(
                        stream_id,
                        'stream',
                        f"Stream {f['stream_id']}",
                        streams.get(f['stream_id'], {})
                    )
                    add_edge(finding_id, stream_id, 'found_in')
            
            # Add sessions and their relationships
            for session_id, session in (sessions or {}).items():
                if not isinstance(session, dict):
                    continue
                    
                session_node_id = f'session_{session_id}'
                add_node(
                    session_node_id,
                    'session',
                    f"Session {session_id}",
                    {
                        'protocol': session.get('protocol'),
                        'src': session.get('src'),
                        'dst': session.get('dst'),
                        'packet_count': session.get('packet_count', 0)
                    }
                )
                
                # Link related streams
                for stream_id, stream in (streams or {}).items():
                    if not isinstance(stream, dict):
                        continue
                    # Simple heuristic: link if IPs match
                    if (session.get('src') == stream.get('src_ip') and 
                        session.get('dst') == stream.get('dst_ip')):
                        stream_node_id = f'stream_{stream_id}'
                        add_node(
                            stream_node_id,
                            'stream',
                            f"Stream {stream_id}",
                            stream
                        )
                        add_edge(session_node_id, stream_node_id, 'contains')
            
            # Build final graph structure
            graph['nodes'] = list(nodes.values())
            graph['edges'] = [
                {
                    'from': src,
                    'to': dst,
                    'type': type_,
                    'id': f"edge_{i}"
                }
                for i, (src, dst, type_) in enumerate(edges)
            ]
            
        except Exception:
            # On any error return empty but valid graph
            graph = {'nodes': [], 'edges': []}
        
        return graph

    def _collect_protocol_details(self, findings: List[Dict[str, Any]], sessions: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Collect individual protocol details for the UI display.

        Returns a list of protocol detail entries, each with keys: 'protocol', 'src_ip', 'dst_ip', 'summary', 'sni'.
        This function is defensive and returns empty list on unexpected input.
        """
        details: List[Dict[str, Any]] = []
        seen_entries = set()  # To avoid duplicates
        
        try:
            # Extract details from findings
            for f in (findings or []):
                proto = f.get('protocol', 'Unknown')
                src_ip = f.get('src_ip', f.get('src', ''))
                dst_ip = f.get('dst_ip', f.get('dst', ''))
                
                # Create entry key to avoid duplicates
                entry_key = f"{proto}|{src_ip}|{dst_ip}"
                if entry_key in seen_entries:
                    continue
                seen_entries.add(entry_key)
                
                # Extract additional details based on finding type
                summary = ""
                sni = ""
                
                if f.get('display_type') == 'JWT':
                    summary = "JWT token detected"
                elif f.get('display_type') == 'FLAG':
                    summary = "Potential flag found"
                elif f.get('display_type') == 'CREDENTIAL':
                    summary = "Credential detected"
                elif f.get('type') == 'http':
                    method = f.get('http_method', '')
                    path = f.get('http_path', '')
                    if method and path:
                        summary = f"{method} {path[:50]}"
                
                # Check for SNI in TLS/HTTPS connections
                if proto.upper() in ['HTTPS', 'TLS']:
                    # Try to extract SNI from data
                    data = str(f.get('data', ''))
                    if '.' in data and len(data.split('.')) > 1:
                        # Simple heuristic for domain-like strings
                        potential_sni = data.split()[0] if ' ' in data else data
                        if len(potential_sni) < 100 and '.' in potential_sni:
                            sni = potential_sni[:50]
                
                details.append({
                    'protocol': proto,
                    'src_ip': src_ip,
                    'dst_ip': dst_ip,
                    'summary': summary,
                    'sni': sni if sni else None
                })

            # Extract details from sessions if available
            if isinstance(sessions, dict):
                for session_id, session in sessions.items():
                    try:
                        if not isinstance(session, dict):
                            continue
                            
                        proto = session.get('protocol', 'Unknown')
                        src_ip = session.get('src', '')
                        dst_ip = session.get('dst', '')
                        
                        # Create entry key to avoid duplicates
                        entry_key = f"{proto}|{src_ip}|{dst_ip}"
                        if entry_key in seen_entries:
                            continue
                        seen_entries.add(entry_key)
                        
                        # Generate summary based on session data
                        summary = ""
                        packet_count = session.get('packet_count', 0)
                        if packet_count > 0:
                            summary = f"{packet_count} packets"
                        
                        # Check for HTTP requests/responses in session
                        if session.get('http_requests') or session.get('http_responses'):
                            req_count = len(session.get('http_requests', []))
                            resp_count = len(session.get('http_responses', []))
                            if req_count > 0 or resp_count > 0:
                                summary = f"{req_count} requests, {resp_count} responses"
                        
                        details.append({
                            'protocol': proto,
                            'src_ip': src_ip,
                            'dst_ip': dst_ip,
                            'summary': summary,
                            'sni': None
                        })
                        
                    except Exception:
                        continue

            # If no details found, add a placeholder entry
            if not details:
                details.append({
                    'protocol': 'No Data',
                    'src_ip': '',
                    'dst_ip': '',
                    'summary': 'No protocol details available',
                    'sni': None
                })
            
            return details[:100]  # Limit to 100 entries to avoid UI performance issues
            
        except Exception:
            # On any error, return a safe default
            return [{
                'protocol': 'Error',
                'src_ip': '',
                'dst_ip': '',
                'summary': 'Error collecting protocol details',
                'sni': None
            }]

    def _detect_and_parse_jwts(self, packet_data_list: List[Dict[str, Any]], decoded_items: List[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """Detect JWT-like tokens in packet data and decoded artifacts and parse their claims.

        Returns a list of dicts with keys: token, header, claims, protocol, src_ip, packet_index
        """
        import re, base64, json

        jwt_regex = re.compile(r'([A-Za-z0-9-_]+)\.([A-Za-z0-9-_]+)\.([A-Za-z0-9-_]+)')
        results = []
        decoded_items = decoded_items or []

        # Combine raw packet data and decoded artifacts for searching
        candidates = []
        for p in (packet_data_list or []):
            # prefer consolidated string fields
            candidates.append({
                'text': p.get('data') or p.get('http_body') or p.get('raw', '') or '',
                'protocol': p.get('protocol', ''),
                'src_ip': p.get('src', ''),
                'packet_index': p.get('packet_index')
            })
        for d in decoded_items:
            candidates.append({
                'text': d.get('decoded') or d.get('result') or d.get('data') or '',
                'protocol': d.get('protocol', ''),
                'src_ip': d.get('src_ip', d.get('src', '')),
                'packet_index': d.get('packet_index')
            })

        def _b64url_decode(s: str) -> bytes:
            s = s.encode('utf-8') if isinstance(s, str) else s
            # pad
            rem = len(s) % 4
            if rem:
                s += b'=' * (4 - rem)
            try:
                return base64.urlsafe_b64decode(s)
            except Exception:
                return b''

        for cand in candidates:
            text = cand.get('text') or ''
            if not text:
                continue
            for m in jwt_regex.finditer(text):
                token = m.group(0)
                parts = token.split('.')
                header = None
                claims = None
                try:
                    header_bytes = _b64url_decode(parts[0])
                    payload_bytes = _b64url_decode(parts[1])
                    try:
                        header = json.loads(header_bytes.decode('utf-8', errors='ignore'))
                    except Exception:
                        header = None
                    try:
                        claims = json.loads(payload_bytes.decode('utf-8', errors='ignore'))
                    except Exception:
                        claims = None
                except Exception:
                    header = None
                    claims = None

                results.append({
                    'token': token,
                    'header': header,
                    'claims': claims,
                    'protocol': cand.get('protocol'),
                    'src_ip': cand.get('src_ip'),
                    'packet_index': cand.get('packet_index')
                })

        return results

    def _identify_potential_flags(self, packet_data_list: List[Dict[str, Any]], custom_regex: Optional[str] = None) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
        """Identify potential flags in packet data, supporting custom regex. Returns (potential_flags, custom_flag_findings)"""
        potential_flags = []
        custom_flag_findings = []
        flag_patterns = [
            r'flag\{[^}]+\}',
            r'CTF\{[^}]+\}',
            r'HTB\{[^}]+\}',
            r'FLAG\{[^}]+\}',
            r'ctf\{[^}]+\}',
            r'htb\{[^}]+\}',
            r'DUCTF\{[^}]+\}',
            r'PICOCTF\{[^}]+\}',
            r'flag:[\s]*[a-zA-Z0-9_\-!@#$%^&*()]+'
        ]
        if custom_regex:
            flag_patterns.insert(0, custom_regex)
        for packet in packet_data_list:
            data = packet.get('data', '')
            if not data:
                continue
            for pattern in flag_patterns:
                matches = re.finditer(pattern, data, re.IGNORECASE | re.MULTILINE)
                for match in matches:
                    match_data = match.group(0)
                    potential_flags.append({
                        'type': 'direct_match',
                        'display_type': 'POTENTIAL FLAG',
                        'icon': 'üö©',
                        'data': match_data,
                        'packet_index': packet.get('packet_index'),
                        'src_ip': packet.get('src', ''),
                        'protocol': packet.get('protocol', 'Unknown'),
                        'confidence': 'high'
                    })
                    if custom_regex and re.fullmatch(custom_regex, match_data):
                        custom_flag_findings.append({
                            'type': 'custom_flag',
                            'display_type': 'CUSTOM FLAG',
                            'icon': 'üö©',
                            'data': match_data,
                            'packet_index': packet.get('packet_index'),
                            'src_ip': packet.get('src', ''),
                            'protocol': packet.get('protocol', 'Unknown'),
                            'confidence': 'high',
                            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                        })
            candidates = set()
            candidates.update(re.findall(r'\b[A-Za-z0-9+/=]{12,}\b', data))
            candidates.update(re.findall(r'\b[0-9A-Fa-f]{16,}\b', data))
            candidates.update(re.findall(r'\b[A-Z2-7]{8,}=*\b', data))
            for word in candidates:
                decoded_variants: List[Tuple[str, str]] = []
                for enc in ['base64', 'base32', 'hex', 'rot13', 'url']:
                    decoder = self.encoding_decoder.decoders.get(enc)
                    if not decoder:
                        continue
                    try:
                        dec = decoder(word)
                        if dec and dec != word:
                            decoded_variants.append((enc, dec))
                    except Exception:
                        pass
                for res in self.encoding_decoder.decode_chain(word):
                    decoded_variants.append(('->'.join(res.get('chain', [])), res.get('decoded', '')))
                for method, dec_txt in decoded_variants:
                    for pattern in flag_patterns:
                        match_obj = re.search(pattern, dec_txt, re.IGNORECASE)
                        if match_obj:
                            match_data = match_obj.group(0)
                            frame_no = (packet.get('packet_index') or 0) + 1
                            extraction_steps = []
                            for part in method.split('->') if method else []:
                                if part == 'base64':
                                    extraction_steps.append({'method': 'base64', 'command': f"echo '<prev>' | base64 -d"})
                                elif part == 'base32':
                                    extraction_steps.append({'method': 'base32', 'command': f"echo '<prev>' | base32 -d"})
                                elif part == 'hex':
                                    extraction_steps.append({'method': 'hex', 'command': f"echo -n '<prev>' | xxd -r -p"})
                                elif part == 'rot13':
                                    extraction_steps.append({'method': 'rot13', 'command': "python -c \"import codecs;print(codecs.decode('<prev>','rot_13'))\""})
                                elif part == 'url':
                                    extraction_steps.append({'method': 'url', 'command': "python -c \"from urllib.parse import unquote;print(unquote('<prev>'))\""})
                                elif part == 'zlib':
                                    extraction_steps.append({'method': 'zlib', 'command': "python -c \"import zlib,sys;print(zlib.decompress(sys.stdin.buffer.read()).decode())\" < decoded.bin"})
                            potential_flags.append({
                                'type': 'encoded_flag',
                                'display_type': 'ENCODED FLAG',
                                'icon': 'üîë',
                                'original': word,
                                'decoded': dec_txt,
                                'encoding': method,
                                'poc': {
                                    'where_found': {
                                        'packet_index': packet.get('packet_index'),
                                        'frame_number': frame_no,
                                        'protocol': packet.get('protocol', 'Unknown'),
                                        'src_ip': packet.get('src', ''),
                                        'dst_ip': packet.get('dst', '')
                                    },
                                    'extraction_steps': extraction_steps if extraction_steps else [{'method': method, 'command': ''}]
                                },
                                'packet_index': packet.get('packet_index'),
                                'src_ip': packet.get('src', ''),
                                'protocol': packet.get('protocol', 'Unknown'),
                                'confidence': 'medium'
                            })
                            if custom_regex and re.fullmatch(custom_regex, match_data):
                                custom_flag_findings.append({
                                    'type': 'custom_flag',
                                    'display_type': 'CUSTOM FLAG',
                                    'icon': 'üö©',
                                    'data': match_data,
                                    'packet_index': packet.get('packet_index'),
                                    'src_ip': packet.get('src', ''),
                                    'protocol': packet.get('protocol', 'Unknown'),
                                    'confidence': 'medium',
                                    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                                })
        return potential_flags, custom_flag_findings

    def _is_potential_flag(self, text: str) -> bool:
        """Check if a string looks like a CTF flag format.
        
        This is a defensive implementation that checks for common flag formats
        without being too aggressive. Returns True if the text matches flag-like
        patterns, False otherwise or on error.
        """
        if not isinstance(text, str):
            return False
        
        try:
            # Common CTF flag formats
            flag_patterns = [
                r'flag\{[^}]+\}',
                r'CTF\{[^}]+\}',
                r'HTB\{[^}]+\}',
                r'FLAG\{[^}]+\}',
                r'DUCTF\{[^}]+\}',
                r'PICOCTF\{[^}]+\}',
                # Simple flags with alphanumeric content
                r'flag:[\s]*[a-zA-Z0-9_\-!@#$%^&*()]+',
                r'key:[\s]*[a-zA-Z0-9_\-!@#$%^&*()]+',
                # Additional cases
                r'\{[0-9a-fA-F]{32}\}',  # MD5-like
                r'\{[0-9a-fA-F]{40}\}',  # SHA1-like
                r'\{[0-9a-fA-F]{64}\}'   # SHA256-like
            ]
            
            # Check each pattern
            for pattern in flag_patterns:
                if re.search(pattern, text, re.IGNORECASE | re.MULTILINE):
                    return True
            
            # Additional heuristics for possible flags
            if len(text) >= 8 and '{' in text and '}' in text:
                inner = text[text.find('{')+1:text.find('}')]
                if len(inner) >= 4:
                    # Look for hex/base64/ascii printable content
                    if all(c in '0123456789abcdefABCDEF' for c in inner):
                        return True
                    if all(c in string.printable for c in inner):
                        return True
            
            return False
        except Exception:
            return False

    def _generate_exploit_suggestions(self, findings: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Generate high-level, safe suggestions based on findings.
        
        This implementation is intentionally conservative and avoids providing
        actual exploit details. It returns investigation suggestions that help
        guide analysis without revealing sensitive details.
        """
        suggestions = []
        
        try:
            # Track what we've seen to avoid duplicate suggestions
            seen_types = set()
            
            for f in (findings or []):
                f_type = f.get('display_type', '').upper()
                if f_type in seen_types:
                    continue
                seen_types.add(f_type)
                
                # JWT token guidance
                if 'JWT' in f_type or (f.get('data') and 'eyJ' in str(f.get('data'))):
                    suggestions.append({
                        'type': 'jwt_analysis',
                        'title': 'JWT Token Analysis',
                        'finding_type': f_type,
                        'risk_level': 'Medium',
                        'suggestion': 'Review JWT claims and headers. Consider checking signature verification and expiration.',
                        'packet_index': f.get('packet_index'),
                        'protocol': f.get('protocol', 'Unknown')
                    })
                
                # Suspicious HTTP patterns
                if f.get('protocol') == 'HTTP' and any(x in str(f.get('data', '')).upper() for x in ['UNION', 'SELECT', 'INSERT', 'DROP']):
                    suggestions.append({
                        'type': 'http_injection',
                        'title': 'HTTP Parameter Analysis',
                        'finding_type': f_type,
                        'risk_level': 'High',
                        'suggestion': 'Review HTTP parameters for potential SQL patterns. Consider input validation.',
                        'packet_index': f.get('packet_index'),
                        'protocol': 'HTTP'
                    })
                
                # Base64/encoded content
                if 'BASE64' in f_type or (f.get('data') and re.search(r'[A-Za-z0-9+/]{30,}={0,2}', str(f.get('data')))):
                    suggestions.append({
                        'type': 'encoded_content',
                        'title': 'Encoded Content Analysis',
                        'finding_type': f_type,
                        'risk_level': 'Low',
                        'suggestion': 'Review base64-encoded content. Consider multi-layer decoding.',
                        'packet_index': f.get('packet_index'),
                        'protocol': f.get('protocol', 'Unknown')
                    })
                
                # Flag format detection
                if 'FLAG' in f_type or self._is_potential_flag(str(f.get('data', ''))):
                    suggestions.append({
                        'type': 'flag_analysis',
                        'title': 'Potential Flag Analysis',
                        'finding_type': f_type,
                        'risk_level': 'Info',
                        'suggestion': 'Verify flag format and consider surrounding packet context.',
                        'packet_index': f.get('packet_index'),
                        'protocol': f.get('protocol', 'Unknown')
                    })
                
                # DNS anomalies
                if f.get('protocol') == 'DNS' and len(str(f.get('data', ''))) > 200:
                    suggestions.append({
                        'type': 'dns_analysis',
                        'title': 'DNS Query Analysis',
                        'finding_type': f_type,
                        'risk_level': 'Medium',
                        'suggestion': 'Review long DNS queries for potential DNS tunneling.',
                        'packet_index': f.get('packet_index'),
                        'protocol': 'DNS'
                    })
                
                # General binary/hex content
                if re.search(r'\\x[0-9a-fA-F]{2}', str(f.get('data', ''))):
                    suggestions.append({
                        'type': 'binary_analysis',
                        'title': 'Binary Content Review',
                        'finding_type': f_type,
                        'risk_level': 'Medium',
                        'suggestion': 'Examine hex-encoded content for potential shellcode or embedded files.',
                        'packet_index': f.get('packet_index'),
                        'protocol': f.get('protocol', 'Unknown')
                    })
            
            # Add protocol-specific suggestions based on what we've seen
            protocols = {f.get('protocol', '').upper() for f in findings if f.get('protocol')}
            
            if 'HTTP' in protocols:
                suggestions.append({
                    'type': 'http_review',
                    'title': 'HTTP Traffic Review',
                    'finding_type': 'PROTOCOL',
                    'risk_level': 'Info',
                    'suggestion': 'Review HTTP headers, cookies, and POST data for sensitive information.',
                    'protocol': 'HTTP'
                })
            
            if 'DNS' in protocols:
                suggestions.append({
                    'type': 'dns_review',
                    'title': 'DNS Query Review',
                    'finding_type': 'PROTOCOL',
                    'risk_level': 'Info',
                    'suggestion': 'Analyze DNS query patterns and subdomain structure.',
                    'protocol': 'DNS'
                })
            
            # Deduplicate by suggestion type
            seen = set()
            unique = []
            for s in suggestions:
                key = f"{s['type']}|{s.get('packet_index', '')}"
                if key not in seen:
                    seen.add(key)
                    unique.append(s)
            
            return unique
        
        except Exception:
            # On any error, return an empty list to keep pipeline running
            return []

    def _detect_and_parse_jwts(self, packet_data_list: List[Dict[str, Any]], decoded_items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Detect JWT tokens in raw and decoded content and pretty-print claims."""
        import re
        import base64 as _b64
        from typing import Optional as _Optional

        def _b64url_to_json(segment: str) -> _Optional[dict]:
            try:
                seg = segment + '=' * (-len(segment) % 4)
                data = _b64.urlsafe_b64decode(seg)
                import json as _json
                return _json.loads(data.decode('utf-8', errors='ignore'))
            except Exception:
                return None

        jwt_re = re.compile(r"eyJ[0-9A-Za-z_-]{10,}\.[0-9A-Za-z_-]{10,}\.[0-9A-Za-z_-]{10,}")
        tokens: List[Dict[str, Any]] = []

        # scan raw packet data
        for p in packet_data_list:
            text = p.get('data', '') or ''
            for m in jwt_re.finditer(text):
                tok = m.group(0)
                parts = tok.split('.')
                if len(parts) != 3:
                    continue
                hdr = _b64url_to_json(parts[0])
                pld = _b64url_to_json(parts[1])
                if hdr and pld:
                    tokens.append({
                        'token': tok,
                        'header': hdr,
                        'claims': pld,
                        'packet_index': p.get('packet_index'),
                        'protocol': p.get('protocol', 'Unknown'),
                        'src_ip': p.get('src', ''),
                        'dst_ip': p.get('dst', '')
                    })

        # scan decoded items
        for d in decoded_items or []:
            text = d.get('decoded', '') or d.get('data', '') or ''
            for m in jwt_re.finditer(text):
                tok = m.group(0)
                parts = tok.split('.')
                if len(parts) != 3:
                    continue
                hdr = _b64url_to_json(parts[0])
                pld = _b64url_to_json(parts[1])
                if hdr and pld:
                    tokens.append({
                        'token': tok,
                        'header': hdr,
                        'claims': pld,
                        'packet_index': d.get('packet_index'),
                        'protocol': d.get('protocol', 'Unknown'),
                        'src_ip': d.get('src_ip', d.get('src', '')),
                        'dst_ip': d.get('dst_ip', d.get('dst', ''))
                    })

        # dedupe by token string
        seen = set()
        deduped = []
        for t in tokens:
            if t['token'] in seen:
                continue
            seen.add(t['token'])
            deduped.append(t)
        return deduped

    def _build_replay_commands(self, sessions: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Generate safe replay commands for traffic reconstruction.
        
        Returns a list of dicts with metadata and commands for replaying traffic.
        This implementation is defensive and focuses on common protocols (HTTP, DNS)
        without including potentially dangerous payloads.
        """
        commands = []
        
        try:
            for session_id, session in (sessions or {}).items():
                if not isinstance(session, dict):
                    continue
                
                proto = (session.get('protocol') or '').upper()
                src = session.get('src', '')
                dst = session.get('dst', '')
                src_port = session.get('src_port')
                dst_port = session.get('dst_port')
                
                if not all([proto, src, dst]):
                    continue
                
                # FTP commands with credential extraction
                if proto == 'FTP' and session.get('ftp_commands'):
                    credentials = {}
                    for cmd_data in session['ftp_commands']:
                        if isinstance(cmd_data, dict):
                            command = cmd_data.get('command', '')
                            args = cmd_data.get('args', '')
                            response = cmd_data.get('response', '')
                            
                            # Extract credentials
                            if command == 'USER':
                                credentials['username'] = args
                            elif command == 'PASS':
                                credentials['password'] = args
                            
                            # Build command description
                            full_cmd = f"{command} {args}" if args else command
                            risk_level = 'high' if command in ['USER', 'PASS', 'STOR', 'DELE'] else 'medium'
                            
                            cmd_entry = {
                                'type': 'ftp_command',
                                'session_id': session_id,
                                'description': f'FTP {command} command',
                                'command': full_cmd,
                                'protocol': 'FTP',
                                'src': src,
                                'dst': dst,
                                'risk_level': risk_level,
                                'packet_number': cmd_data.get('packet_number', 'N/A')
                            }
                            
                            # Add credentials if found
                            if credentials:
                                cmd_entry['credentials'] = credentials.copy()
                            
                            # Add response info
                            if response:
                                cmd_entry['response'] = response
                                
                            commands.append(cmd_entry)
                
                # SSH/Telnet login attempts
                if proto in ['SSH', 'TELNET'] and session.get('login_attempts'):
                    for attempt in session['login_attempts']:
                        if isinstance(attempt, dict):
                            username = attempt.get('username', '')
                            password = attempt.get('password', '')
                            
                            cmd_entry = {
                                'type': f'{proto.lower()}_login',
                                'session_id': session_id,
                                'description': f'{proto} login attempt',
                                'command': f'{proto.lower()} {username}@{dst}',
                                'protocol': proto,
                                'src': src,
                                'dst': dst,
                                'risk_level': 'high',
                                'credentials': {'username': username, 'password': password}
                            }
                            commands.append(cmd_entry)
                
                # HTTP authentication
                if proto == 'HTTP' and isinstance(session.get('http_requests'), list):
                    for req in session['http_requests']:
                        if not isinstance(req, (str, bytes)):
                            continue
                            
                        try:
                            # Parse for authentication headers
                            if isinstance(req, bytes):
                                req = req.decode('utf-8', errors='ignore')
                            
                            # Check for Authorization header
                            auth_header = None
                            for line in req.split('\n'):
                                if line.lower().startswith('authorization:'):
                                    auth_header = line
                                    break
                            
                            first_line = req.split('\n')[0].strip()
                            method, path, _ = first_line.split(' ', 2)
                            
                            cmd_entry = {
                                'type': 'http_request',
                                'session_id': session_id,
                                'description': f'HTTP {method} request to {dst}:{dst_port}',
                                'command': f'curl -X {method} -v "http://{dst}:{dst_port}{path}"',
                                'protocol': 'HTTP',
                                'src': src,
                                'dst': dst,
                                'risk_level': 'medium' if auth_header else 'low'
                            }
                            
                            # Add authentication info if found
                            if auth_header:
                                cmd_entry['credentials'] = {'auth_header': auth_header}
                                cmd_entry['command'] += f' -H "{auth_header}"'
                            
                            commands.append(cmd_entry)
                        except Exception:
                            continue
                
                # DNS queries
                if proto == 'DNS':
                    base_cmd = "nslookup"  # Default to simple nslookup
                    if session.get('dns_queries'):
                        for query in session['dns_queries']:
                            if isinstance(query, str) and len(query) < 255:  # Basic safety check
                                commands.append({
                                    'type': 'dns_query',
                                    'session_id': session_id,
                                    'description': f'DNS query for {query}',
                                    'command': f'{base_cmd} {query} {dst}',
                                    'protocol': 'DNS',
                                    'src': src,
                                    'dst': dst,
                                    'risk_level': 'low'
                                })
                
                # TCP connections (netcat-style, without payloads)
                if proto == 'TCP' and src_port and dst_port:
                    commands.append({
                        'type': 'tcp_connect',
                        'session_id': session_id,
                        'description': f'TCP connection to {dst}:{dst_port}',
                        'command': f'nc -v -w 5 {dst} {dst_port}',
                        'protocol': 'TCP',
                        'src': src,
                        'dst': dst,
                        'risk_level': 'medium'
                    })
                
                # ICMP ping (if ICMP traffic seen)
                if proto == 'ICMP':
                    commands.append({
                        'type': 'icmp_ping',
                        'session_id': session_id,
                        'description': f'ICMP ping to {dst}',
                        'command': f'ping -c 4 {dst}',
                        'protocol': 'ICMP',
                        'src': src,
                        'dst': dst,
                        'risk_level': 'low'
                    })
            
            # Add general tcpreplay/tshark commands if we have any sessions
            if sessions:
                commands.append({
                    'type': 'pcap_replay',
                    'description': 'Replay entire capture (requires tcpreplay)',
                    'command': 'tcpreplay -i <interface> capture.pcap',
                    'protocol': 'ALL',
                    'risk_level': 'high'
                })
                
                commands.append({
                    'type': 'pcap_analysis',
                    'description': 'Detailed protocol analysis (requires tshark)',
                    'command': 'tshark -r capture.pcap -q -z io,phs',
                    'protocol': 'ALL',
                    'risk_level': 'low'
                })
        
        except Exception:
            # On any error, return empty list to keep analysis robust
            return []
        
        return commands

    def _extract_voip_audio(self, sessions: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Extract audio data from VoIP/RTP sessions.
        
        Returns a list of dicts with VoIP call metadata and audio data.
        This implementation focuses on RTP stream identification and basic
        payload extraction, returning safe metadata even if audio processing fails.
        """
        voip_data = []
        
        try:
            from scapy.all import RTP
            
            # Process each session
            for session_id, session in (sessions or {}).items():
                if not isinstance(session, dict):
                    continue
                    
                # Look for RTP packets in the session
                rtp_packets = []
                packets = session.get('packets', [])
                
                for pkt in packets:
                    try:
                        if pkt.haslayer(RTP):
                            rtp = pkt[RTP]
                            rtp_packets.append({
                                'timestamp': getattr(rtp, 'timestamp', 0),
                                'sequence': getattr(rtp, 'sequence', 0),
                                'payload_type': getattr(rtp, 'payload_type', 0),
                                'payload': bytes(rtp.payload) if hasattr(rtp, 'payload') else b'',
                                'marker': bool(getattr(rtp, 'marker', 0))
                            })
                    except Exception:
                        continue
                
                if not rtp_packets:
                    continue
                
                # Group by payload type (different streams/codecs)
                by_payload = {}
                for p in rtp_packets:
                    pt = p['payload_type']
                    if pt not in by_payload:
                        by_payload[pt] = []
                    by_payload[pt].append(p)
                
                # Process each payload type stream
                for payload_type, packets in by_payload.items():
                    # Sort by sequence number
                    packets.sort(key=lambda x: x['sequence'])
                    
                    # Combine payloads (up to reasonable size)
                    MAX_AUDIO_SIZE = 1024 * 1024  # 1MB limit
                    combined = b''
                    for p in packets:
                        if len(combined) + len(p['payload']) > MAX_AUDIO_SIZE:
                            break
                        combined += p['payload']
                    
                    # Basic codec detection
                    codec_info = {
                        0: {'name': 'PCMU', 'rate': 8000, 'channels': 1},  # G.711 ¬µ-law
                        8: {'name': 'PCMA', 'rate': 8000, 'channels': 1},  # G.711 A-law
                        3: {'name': 'GSM', 'rate': 8000, 'channels': 1},
                        9: {'name': 'G722', 'rate': 16000, 'channels': 1},
                        18: {'name': 'G729', 'rate': 8000, 'channels': 1}
                    }.get(payload_type, {'name': f'Unknown-{payload_type}', 'rate': 8000, 'channels': 1})
                    
                    voip_data.append({
                        'session_id': session_id,
                        'src_ip': session.get('src', ''),
                        'dst_ip': session.get('dst', ''),
                        'codec': codec_info['name'],
                        'sample_rate': codec_info['rate'],
                        'channels': codec_info['channels'],
                        'packets': len(packets),
                        'duration_ms': (packets[-1]['timestamp'] - packets[0]['timestamp']) / codec_info['rate'] * 1000 if packets else 0,
                        'payload_type': payload_type,
                        'raw_audio': combined if len(combined) <= MAX_AUDIO_SIZE else None,
                        'raw_size': len(combined)
                    })
        
        except Exception:
            # On any error, return empty list to keep analysis robust
            return []
        
        return voip_data

    def _extract_patterns_from_packets(self, packet_data_list: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Conservative pattern extraction from packet data.

        Returns a list of dicts with keys: 'pattern', 'type', 'packet_index', 'confidence', 'data'.
        Uses `self.pattern_extractor` when available, otherwise falls back to safe regex heuristics.
        """
        patterns: List[Dict[str, Any]] = []
        if not packet_data_list:
            return patterns

        try:
            for p in packet_data_list:
                data = p.get('data', '') if isinstance(p, dict) else ''
                if not data:
                    continue
                pkt_idx = p.get('packet_index', -1) if isinstance(p, dict) else -1
                timestamp = p.get('timestamp', datetime.now().strftime('%Y-%m-%d %H:%M:%S'))

                # Helper function to normalize findings
                def normalize_finding(finding):
                    if isinstance(finding, str):
                        return {'data': finding, 'type': 'raw'}
                    elif isinstance(finding, dict):
                        return finding
                    return None

                # Prefer an existing pattern_extractor if it provides an extract() method
                if hasattr(self, 'pattern_extractor') and getattr(self.pattern_extractor, 'extract', None):
                    try:
                        extracted = self.pattern_extractor.extract(data)
                        if isinstance(extracted, list):
                            for e in extracted:
                                normalized = normalize_finding(e)
                                if normalized:
                                    normalized.setdefault('packet_index', pkt_idx)
                                    normalized.setdefault('timestamp', timestamp)
                                    normalized.setdefault('display_type', normalized.get('type', 'PATTERN').upper())
                                    normalized.setdefault('icon', 'üîç')
                                    patterns.append(normalized)
                            continue
                    except Exception:
                        # Fall through to regex heuristics on extractor failure
                        pass

                # Fallback heuristics (conservative)
                # Create a base pattern dict with common fields
                def create_pattern_dict(pattern, ptype, confidence):
                    return {
                        'pattern': pattern,
                        'type': ptype,
                        'packet_index': pkt_idx,
                        'confidence': confidence,
                        'data': pattern,
                        'display_type': ptype.upper(),
                        'icon': {
                            'base64': 'üìù',
                            'hex': 'üî¢',
                            'url': 'üåê',
                            'email': 'üìß'
                        }.get(ptype, 'üîç')
                    }

                # Base64-like long strings
                for m in re.findall(r'([A-Za-z0-9+/]{40,}={0,2})', str(data)):
                    patterns.append(create_pattern_dict(m, 'base64', 65))

                # Long hex sequences
                for m in re.findall(r'\b([0-9a-fA-F]{16,})\b', str(data)):
                    patterns.append(create_pattern_dict(m, 'hex', 60))

                # URLs
                for m in re.findall(r'https?://[^\s\'\"]+', str(data)):
                    patterns.append(create_pattern_dict(m, 'url', 80))

                # Email addresses
                for m in re.findall(r'[\w\.-]+@[\w\.-]+\.\w+', str(data)):
                    patterns.append(create_pattern_dict(m, 'email', 90))

        except Exception:
            # On unexpected errors return empty list to keep analysis robust
            return []

        # Final pass to ensure all patterns have required fields
        normalized_patterns = []
        for pattern in patterns:
            if not isinstance(pattern, dict):
                continue
                
            # Ensure all required fields are present with defaults
            normalized = {
                'type': pattern.get('type', 'unknown'),
                'display_type': pattern.get('display_type', pattern.get('type', 'PATTERN').upper()),
                'icon': pattern.get('icon', 'üîç'),
                'data': pattern.get('data', pattern.get('pattern', '')),
                'packet_index': pattern.get('packet_index', -1),
                'confidence': pattern.get('confidence', 50),
                'timestamp': pattern.get('timestamp', datetime.now().strftime('%Y-%m-%d %H:%M:%S')),
                'context': pattern.get('context', '')
            }
            
            normalized_patterns.append(normalized)

        return normalized_patterns
    
    def _generate_hints(self) -> List[str]:
        """Generate hints based on findings"""
        hints = []
        
        # Check if we found first letters patterns
        if any(item.get('type') == 'first_letters' for item in self.results.get('extracted_patterns', [])):
            hints.append("üî§ Found patterns of first letters - try combining them to form a message")
        
        # Check if we found base64 encoded data
        if any(item.get('type') == 'base64' for item in self.results.get('decoded_data', [])):
            hints.append("üîê Found base64 encoded data - check decoded content for hidden messages")
        
        # Check if we found potential flags
        if self.results.get('potential_flags', []):
            hints.append("üö© Found potential flags - verify if they match the expected format")
        
        # Add general hints for CTF challenges
        hints.extend([
            "üîç Look for patterns across multiple packets",
            "üåê Check for unusual HTTP headers or request patterns",
            "üìä Analyze traffic patterns for anomalies",
            "üß© Try combining findings from different packets"
        ])
        
        return hints
    
    def _identify_suspicious_packets(self, packet_data_list: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Identify suspicious packets for further investigation"""
        suspicious_packets = []
        
        for packet in packet_data_list:
            # Skip if no data
            if not packet.get('data'):
                continue
            
            data = packet.get('data', '')
            is_suspicious = False
            reasons = []
            
            # Check for unusual HTTP methods
            if packet.get('protocol') == 'HTTP' and 'http_headers' in packet:
                headers = packet['http_headers']
                if re.search(r'^(PUT|DELETE|TRACE|CONNECT|OPTIONS|PATCH)', headers):
                    is_suspicious = True
                    reasons.append("Unusual HTTP method")
            
            # Check for potential command injection
            if re.search(r'[;|`]\s*[a-zA-Z]+', data):
                is_suspicious = True
                reasons.append("Potential command injection")
            
            # Check for potential SQL injection
            if re.search(r"['\"\-]\s*OR\s*['\"\-]|UNION\s+SELECT|INSERT\s+INTO|DROP\s+TABLE", data, re.IGNORECASE):
                is_suspicious = True
                reasons.append("Potential SQL injection")
            
            # Check for potential XSS
            if re.search(r'<script>|javascript:|onerror=|onload=', data, re.IGNORECASE):
                is_suspicious = True
                reasons.append("Potential XSS")
            
            # Check for base64 executable content
            if re.search(r'TVqQAAMAAAAEAAAA', data) or re.search(r'UEsDBBQAA', data):
                is_suspicious = True
                reasons.append("Base64 encoded executable")
            
            # Check for potential CTF flags
            if re.search(r'flag\{[^}]+\}|CTF\{[^}]+\}|HTB\{[^}]+\}', data, re.IGNORECASE):
                is_suspicious = True
                reasons.append("Contains potential flag format")
            
            # Add to suspicious packets if any checks triggered
            if is_suspicious:
                suspicious_packets.append({
                    'packet_index': packet.get('packet_index'),
                    'protocol': packet.get('protocol', 'Unknown'),
                    'src_ip': packet.get('src', ''),
                    'dst_ip': packet.get('dst', ''),
                    'reasons': reasons,
                    'data_preview': data[:100] + '...' if len(data) > 100 else data
                })
        
        return suspicious_packets
    
    def export_results(self, format_type: str = 'json') -> str:
        """Export results in specified format"""
        
        if format_type.lower() == 'json':
            return json.dumps(self.results, indent=2, default=str)
        
        elif format_type.lower() == 'csv':
            import pandas as pd
            
            # Convert findings to DataFrame
            df_data = []
            for finding in self.results['findings']:
                df_data.append({
                    'Type': finding.get('display_type', ''),
                    'Protocol': finding.get('protocol', ''),
                    'Source IP': finding.get('src_ip', ''),
                    'Destination IP': finding.get('dst_ip', ''),
                    'Content': finding.get('data', ''),
                    'Context': finding.get('context', ''),
                    'Timestamp': finding.get('timestamp', '')
                })
            
            df = pd.DataFrame(df_data)
            return df.to_csv(index=False)
        
        elif format_type.lower() == 'html':
            return self._generate_html_report()
        
        elif format_type.lower() == 'pdf':
            return self._generate_pdf_report()
        
        else:
            raise ValueError(f"Unsupported export format: {format_type}")
            
    def _generate_pdf_report(self) -> bytes:
        """Generate a PDF report of the analysis results"""
        try:
            from fpdf import FPDF
            import io
            from datetime import datetime
            
            # Create PDF object
            pdf = FPDF()
            pdf.add_page()
            
            # Set up fonts
            pdf.set_font("Arial", "B", 16)
            
            # Title
            pdf.cell(0, 10, "FlagSniff Analysis Report", 0, 1, "C")
            pdf.set_font("Arial", "I", 10)
            pdf.cell(0, 10, f"Generated on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}", 0, 1, "C")
            pdf.ln(5)
            
            # Statistics section
            pdf.set_font("Arial", "B", 14)
            pdf.cell(0, 10, "Analysis Statistics", 0, 1, "L")
            pdf.set_font("Arial", "", 10)
            
            # Create statistics table
            stats = [
                ["Total Packets", str(self.results.get('stats', {}).get('total_packets', 0))],
                ["Analyzed Packets", str(self.results.get('stats', {}).get('analyzed_packets', 0))],
                ["HTTP Requests", str(self.results.get('stats', {}).get('http_requests', 0))],
                ["HTTP Responses", str(self.results.get('stats', {}).get('http_responses', 0))],
                ["Suspicious Packets", str(self.results.get('stats', {}).get('suspicious_packets', 0))],
                ["Potential Flags", str(len([f for f in self.results.get('findings', []) if f.get('display_type') == 'Flag']))]
            ]
            
            # Add statistics table
            for stat in stats:
                pdf.cell(80, 8, stat[0], 1)
                pdf.cell(80, 8, stat[1], 1)
                pdf.ln()
            
            pdf.ln(10)
            
            # Findings section
            pdf.set_font("Arial", "B", 14)
            pdf.cell(0, 10, "Key Findings", 0, 1, "L")
            
            # Add findings
            findings = self.results.get('findings', [])
            for i, finding in enumerate(findings):
                if i > 0:
                    pdf.add_page()  # New page for each finding after the first
                
                # Finding header
                pdf.set_font("Arial", "B", 12)
                pdf.cell(0, 10, f"Finding #{i+1}: {finding.get('display_type', 'Unknown')}", 0, 1, "L")
                
                # Finding details
                pdf.set_font("Arial", "", 10)
                pdf.cell(40, 8, "Protocol:", 0)
                pdf.cell(0, 8, finding.get('protocol', 'Unknown'), 0, 1)
                
                pdf.cell(40, 8, "Source IP:", 0)
                pdf.cell(0, 8, finding.get('src_ip', 'Unknown'), 0, 1)
                
                pdf.cell(40, 8, "Destination IP:", 0)
                pdf.cell(0, 8, finding.get('dst_ip', 'Unknown'), 0, 1)
                
                pdf.cell(40, 8, "Timestamp:", 0)
                pdf.cell(0, 8, str(finding.get('timestamp', 'Unknown')), 0, 1)
                
                # Finding content
                pdf.ln(5)
                pdf.set_font("Arial", "B", 10)
                pdf.cell(0, 8, "Content:", 0, 1)
                pdf.set_font("Arial", "", 9)
                
                # Handle multiline content
                content = finding.get('data', '')
                if len(content) > 80:  # Wrap long content
                    wrapped_content = [content[i:i+80] for i in range(0, len(content), 80)]
                    for line in wrapped_content:
                        pdf.multi_cell(0, 5, line)
                else:
                    pdf.multi_cell(0, 5, content)
                
                pdf.ln(5)
                
                # Context if available
                if finding.get('context'):
                    pdf.set_font("Arial", "B", 10)
                    pdf.cell(0, 8, "Context:", 0, 1)
                    pdf.set_font("Arial", "", 9)
                    pdf.multi_cell(0, 5, finding.get('context', ''))
                
                pdf.ln(5)
            
            # Summary section if available
            if self.results.get('summary'):
                pdf.add_page()
                pdf.set_font("Arial", "B", 14)
                pdf.cell(0, 10, "Analysis Summary", 0, 1, "L")
                pdf.set_font("Arial", "", 10)
                pdf.multi_cell(0, 5, self.results.get('summary', ''))
            
            # Output PDF to bytes
            pdf_bytes = io.BytesIO()
            pdf.output(pdf_bytes)
            pdf_bytes.seek(0)
            return pdf_bytes.getvalue()
            
        except ImportError:
            # Fallback if fpdf is not available
            return bytes(f"PDF generation requires fpdf library. Please install with 'pip install fpdf2'.", 'utf-8')
    
    def _generate_html_report(self) -> str:
        """Generate detailed HTML report"""
        
        html_template = """
        <!DOCTYPE html>
        <html>
        <head>
            <title>FlagSniff Analysis Report</title>
            <meta charset="UTF-8">
            <style>
                body {{ 
                    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; margin: 0; padding: 20px; background: #f5f5f5; 
                }}
                .container {{ max-width: 1200px; margin: 0 auto; background: white; padding: 30px; border-radius: 10px; box-shadow: 0 4px 6px rgba(0,0,0,0.1); }}
                .header {{ 
                    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); 
                    color: white; padding: 30px; border-radius: 10px; text-align: center; margin-bottom: 30px;
                }}
                .stats-grid {{ display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 20px; margin: 30px 0; }}
                .stat-card {{ 
                    background: #f8f9fa; padding: 20px; border-radius: 8px; text-align: center; 
                    border-left: 4px solid #667eea;
                }}
                .stat-number {{ font-size: 2em; font-weight: bold; color: #667eea; }}
                .stat-label {{ color: #666; margin-top: 5px; }}
                .findings-section {{ margin: 30px 0; }}
                .finding-item {{ 
                    background: #fff; border: 1px solid #e9ecef; border-radius: 8px; 
                    margin: 10px 0; padding: 20px; border-left: 4px solid #28a745;
                }}
                .finding-header {{ display: flex; justify-content: space-between; align-items: center; margin-bottom: 10px; }}
                .finding-type {{ 
                    background: #667eea; color: white; padding: 5px 15px; 
                    border-radius: 20px; font-size: 0.9em; font-weight: bold;
                }}
                .finding-content {{ 
                    background: #f8f9fa; padding: 15px; border-radius: 5px; font-family: 'Courier New', monospace; word-break: break-all;
                }}
                .meta-info {{ color: #666; font-size: 0.9em; margin-top: 10px; }}
                table {{ width: 100%; border-collapse: collapse; margin: 20px 0; }}
            </style>
        </head>
        <body>
            <div class="container">
                <div class="header">
                    <h1>FlagSniff Analysis Report</h1>
                    <p>Generated on {timestamp}</p>
                </div>
                
                <div class="stats-grid">
                    <div class="stat-card">
                        <div class="stat-number">{total_packets}</div>
                        <div class="stat-label">Total Packets</div>
                    </div>
                    <div class="stat-card">
                        <div class="stat-number">{analyzed_packets}</div>
                        <div class="stat-label">Analyzed Packets</div>
                    </div>
                    <div class="stat-card">
                        <div class="stat-number">{findings_count}</div>
                        <div class="stat-label">Findings</div>
                    </div>
                </div>
                
                <div class="findings-section">
                    <h2>Findings</h2>
                    {findings_html}
                </div>
            </div>
        </body>
        </html>
        """
        
        # Generate findings HTML
        findings_html = ""
        for finding in self.results.get('findings', []):
            findings_html += f"""
            <div class="finding-item">
                <div class="finding-header">
                    <div class="finding-type">{finding.get('display_type', 'Unknown')}</div>
                    <div>{finding.get('protocol', '')}</div>
                </div>
                <div class="finding-content">{finding.get('data', '')}</div>
                <div class="meta-info">
                    Source: {finding.get('src_ip', '')} ‚Üí Destination: {finding.get('dst_ip', '')}
                    <br>Timestamp: {finding.get('timestamp', '')}
                </div>
            </div>
            """
        
        # Fill in template
        from datetime import datetime
        html_report = html_template.format(
            timestamp=datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            total_packets=self.results.get('total_packets', 0),
            analyzed_packets=self.results.get('analyzed_packets', 0),
            findings_count=len(self.results.get('findings', [])),
            findings_html=findings_html
        )
        
        return html_report
        
    def _generate_pdf_report(self) -> bytes:
        """Generate PDF report of analysis results"""
        try:
            from fpdf import FPDF
            import io
            
            # Create PDF object
            pdf = FPDF()
            pdf.add_page()
            
            # Set styles
            pdf.set_font("Arial", "B", 16)
            pdf.set_fill_color(102, 126, 234)  # Header background color
            pdf.set_text_color(255, 255, 255)  # Header text color
            
            # Header
            pdf.cell(0, 20, "FlagSniff Analysis Report", 1, 1, "C", True)
            pdf.set_font("Arial", "I", 10)
            pdf.set_text_color(0, 0, 0)
            pdf.cell(0, 10, f"Generated on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}", 0, 1, "C")
            
            # Stats
            pdf.ln(10)
            pdf.set_font("Arial", "B", 14)
            pdf.cell(0, 10, "Analysis Statistics", 0, 1, "L")
            pdf.set_font("Arial", "", 12)
            
            # Create stats table
            pdf.set_fill_color(240, 240, 240)
            pdf.cell(60, 10, "Total Packets", 1, 0, "L", True)
            pdf.cell(0, 10, f"{self.results.get('total_packets', 0):,}", 1, 1, "L")
            
            pdf.cell(60, 10, "Analyzed Packets", 1, 0, "L", True)
            pdf.cell(0, 10, f"{self.results.get('analyzed_packets', 0):,}", 1, 1, "L")
            
            pdf.cell(60, 10, "Findings", 1, 0, "L", True)
            pdf.cell(0, 10, f"{len(self.results.get('findings', []))}", 1, 1, "L")
            
            # Findings
            pdf.ln(10)
            pdf.set_font("Arial", "B", 14)
            pdf.cell(0, 10, "Findings", 0, 1, "L")
            
            # Add findings
            for i, finding in enumerate(self.results.get('findings', [])):
                # Add page break if needed
                if i > 0 and pdf.get_y() > 220:
                    pdf.add_page()
                
                pdf.set_font("Arial", "B", 12)
                pdf.set_fill_color(102, 126, 234)
                pdf.set_text_color(255, 255, 255)
                pdf.cell(0, 10, f"{finding.get('display_type', 'Unknown')} ({finding.get('protocol', '')})", 1, 1, "L", True)
                
                pdf.set_font("Arial", "", 10)
                pdf.set_text_color(0, 0, 0)
                
                # Source and destination
                pdf.cell(40, 8, "Source IP:", 1, 0, "L", True)
                pdf.cell(0, 8, finding.get('src_ip', ''), 1, 1, "L")
                
                pdf.cell(40, 8, "Destination IP:", 1, 0, "L", True)
                pdf.cell(0, 8, finding.get('dst_ip', ''), 1, 1, "L")
                
                # Content
                pdf.cell(0, 8, "Content:", 1, 1, "L", True)
                
                # Handle long content with wrapping
                content = finding.get('data', '')
                pdf.multi_cell(0, 8, content, 1, "L")
                
                # Timestamp
                pdf.cell(40, 8, "Timestamp:", 1, 0, "L", True)
                pdf.cell(0, 8, str(finding.get('timestamp', '')), 1, 1, "L")
                
                pdf.ln(5)
            
            # Output PDF to bytes
            pdf_bytes = io.BytesIO()
            pdf.output(pdf_bytes)
            pdf_bytes.seek(0)
            return pdf_bytes.getvalue()
            
        except Exception as e:
            import traceback
            traceback.print_exc()
            raise ValueError(f"Failed to generate PDF: {str(e)}")

    def _carve_files_from_streams(self, streams):
        """Extract files from TCP/UDP streams using file signatures"""
        carved_files = []
        for stream_id, stream in streams.items():
            data = stream.get('data', b'')
            if not data:
                continue
            for signature, file_info in self.file_signatures.items():
                start_pos = 0
                while True:
                    pos = data.find(signature, start_pos)
                    if pos == -1:
                        break
                    file_data = self._extract_file_from_position(data, pos, file_info)
                    if file_data:
                        file_hash = hashlib.md5(file_data).hexdigest()
                        carved_file = {
                            'stream_id': stream_id,
                            'file_type': file_info['name'],
                            'extension': file_info['ext'],
                            'size': len(file_data),
                            'md5_hash': file_hash,
                            'position': pos,
                            'filename': f"carved_{file_hash[:8]}.{file_info['ext']}",
                            'source_protocol': stream.get('protocol', 'Unknown'),
                            'source_ips': [stream.get('src_ip'), stream.get('dst_ip')],
                            'data': file_data
                        }
                        carved_files.append(carved_file)
                    start_pos = pos + 1
        # De-duplicate carved files by (hash, position, extension, stream)
        unique = {}
        for f in carved_files:
            key = (f.get('md5_hash'), f.get('position'), f.get('extension'), f.get('stream_id'))
            if key not in unique:
                unique[key] = f
        return list(unique.values())

    def _extract_file_from_position(self, data, pos, file_info):
        try:
            if file_info['ext'] in ['png', 'jpg', 'gif']:
                if file_info['ext'] == 'png':
                    end_marker = b'\x00\x00\x00\x00IEND\xaeB`\x82'
                elif file_info['ext'] == 'jpg':
                    end_marker = b'\xff\xd9'
                else:
                    end_marker = b'\x00\x3b'
                end_pos = data.find(end_marker, pos)
                if end_pos != -1:
                    return data[pos:end_pos + len(end_marker)]
            elif file_info['ext'] == 'pdf':
                end_pos = data.find(b'%%EOF', pos)
                if end_pos != -1:
                    return data[pos:end_pos + 5]
            elif file_info['ext'] == 'zip':
                return data[pos:pos + 1024]
            else:
                return data[pos:pos + 1024]
        except Exception:
            return None

    def _analyze_carved_files(self, carved_files: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        enriched = []
        for f in carved_files:
            data = f.get('data', b'') or b''
            ext = (f.get('extension') or f.get('ext') or '').lower()
            analysis = {'metadata': {}, 'stego': [], 'strings': []}
            try:
                # EXIF/metadata for images
                if ext in ('png', 'jpg', 'jpeg', 'gif', 'webp', 'bmp') and data:
                    try:
                        from PIL import Image, ExifTags
                        img = Image.open(io.BytesIO(data))
                        analysis['metadata']['format'] = getattr(img, 'format', '')
                        analysis['metadata']['size'] = getattr(img, 'size', '')
                        analysis['metadata']['mode'] = getattr(img, 'mode', '')
                        if hasattr(img, '_getexif') and img._getexif():
                            raw_exif = img._getexif() or {}
                            exif_info = {}
                            for tag, val in raw_exif.items():
                                name = ExifTags.TAGS.get(tag, str(tag))
                                exif_info[name] = str(val)[:200]
                            if exif_info:
                                analysis['metadata']['exif'] = exif_info
                    except Exception:
                        pass
                
                # PNG chunk scan and text extraction
                if ext == 'png' and data:
                    try:
                        # Scan for hidden text in PNG chunks
                        stego_text = self._png_lsb_extract(data)
                        if stego_text:
                            analysis['stego'].append({
                                'type': 'lsb',
                                'content': stego_text[:500],
                                'method': 'PNG LSB Steganography'
                            })
                        
                        # Extract PNG chunks for analysis
                        chunks = self._extract_png_chunks(data)
                        if chunks:
                            analysis['metadata']['png_chunks'] = chunks
                    except Exception:
                        pass
                
                # ZIP file analysis
                if ext == 'zip' and data:
                    try:
                        import zipfile
                        import io
                        zip_io = io.BytesIO(data)
                        with zipfile.ZipFile(zip_io, 'r') as zip_file:
                            file_list = zip_file.namelist()
                            analysis['metadata']['zip_files'] = file_list[:20]  # Limit to first 20 files
                            analysis['metadata']['zip_file_count'] = len(file_list)
                            
                            # Look for interesting files in the ZIP
                            interesting_patterns = [
                                r'flag', r'password', r'key', r'secret', r'hidden',
                                r'\.(txt|doc|pdf|jpg|png|gif)$'
                            ]
                            
                            interesting_files = []
                            for filename in file_list:
                                for pattern in interesting_patterns:
                                    if re.search(pattern, filename, re.IGNORECASE):
                                        interesting_files.append(filename)
                                        break
                            
                            if interesting_files:
                                analysis['metadata']['interesting_files'] = interesting_files
                                
                            # Extract and analyze text files within the ZIP
                            for filename in file_list:
                                if filename.lower().endswith(('.txt', '.doc', '.rtf', '.md', '.csv')):
                                    try:
                                        with zip_file.open(filename) as file:
                                            content = file.read().decode('utf-8', errors='ignore')
                                            # Look for flags in the content
                                            flag_patterns = [
                                                r'[A-Z0-9_]+\{[^\}]{4,}\}',  # CTF flag format
                                                r'flag\s*:.*',  # flag: format
                                                r'password\s*:.*',  # password: format
                                                r'secret\s*:.*',  # secret: format
                                            ]
                                            
                                            for pattern in flag_patterns:
                                                matches = re.findall(pattern, content, re.IGNORECASE)
                                                for match in matches:
                                                    analysis['stego'].append({
                                                        'type': 'hidden_string',
                                                        'content': match,
                                                        'method': f'Found in ZIP file {filename}',
                                                        'filename': filename
                                                    })
                                    except Exception:
                                        pass
                    except Exception:
                        pass
                
                # Extract strings from binary files
                if data and ext not in ['txt', 'html', 'css', 'js', 'json', 'xml']:
                    try:
                        # Extract printable strings
                        strings = self._extract_strings(data)
                        if strings:
                            # Filter for potentially interesting strings
                            interesting_strings = []
                            flag_patterns = [
                                r'[A-Z0-9_]+\{[^\}]{4,}\}',  # CTF flag format
                                r'flag\s*:.*',  # flag: format
                                r'password\s*:.*',  # password: format
                                r'secret\s*:.*',  # secret: format
                                r'[A-Za-z0-9+/]{20,}={0,2}',  # Base64 strings
                            ]
                            
                            for s in strings[:100]:  # Limit to first 100 strings
                                # Check for flags
                                for pattern in flag_patterns:
                                    if re.search(pattern, s, re.IGNORECASE):
                                        analysis['stego'].append({
                                            'type': 'hidden_string',
                                            'content': s,
                                            'method': 'String Analysis'
                                        })
                                        break
                                else:
                                    # Add long strings that might be interesting
                                    if len(s) > 20:
                                        interesting_strings.append(s)
                            
                            if interesting_strings:
                                analysis['strings'] = interesting_strings[:20]  # Limit to 20 strings
                    except Exception:
                        pass
                
                # Add analysis to file object
                f['analysis'] = analysis
                
            except Exception as e:
                # Continue with basic file info even if analysis fails
                f['analysis'] = analysis
            
            enriched.append(f)
        
        return enriched

    def _extract_strings(self, data: bytes, min_length: int = 4) -> List[str]:
        """Extract printable strings from binary data"""
        try:
            # Convert bytes to string, replacing non-printable chars
            text = ''.join(chr(b) if 32 <= b <= 126 else '\x00' for b in data)
            # Split on null bytes and filter by length
            strings = [s for s in text.split('\x00') if len(s) >= min_length]
            return strings
        except Exception:
            return []

    def _extract_png_chunks(self, data: bytes) -> List[Dict[str, Any]]:
        """Extract PNG chunks for analysis"""
        chunks = []
        try:
            if not data.startswith(b'\x89PNG\r\n\x1a\n'):
                return chunks
            
            pos = 8  # Skip PNG signature
            while pos < len(data) - 8:
                # Read chunk length
                if pos + 4 > len(data):
                    break
                length = int.from_bytes(data[pos:pos+4], 'big')
                pos += 4
                
                # Read chunk type
                if pos + 4 > len(data):
                    break
                chunk_type = data[pos:pos+4].decode('ascii', errors='ignore')
                pos += 4
                
                # Read chunk data
                if pos + length > len(data):
                    break
                chunk_data = data[pos:pos+length]
                pos += length
                
                # Read CRC
                if pos + 4 > len(data):
                    break
                crc = data[pos:pos+4]
                pos += 4
                
                chunks.append({
                    'type': chunk_type,
                    'length': length,
                    'data_size': len(chunk_data),
                    'is_critical': chunk_type.isupper(),
                    'is_public': chunk_type[1].isupper()
                })
                
                # Stop at IEND chunk
                if chunk_type == 'IEND':
                    break
                    
        except Exception:
            pass
            
        return chunks

    def _png_lsb_extract(self, data: bytes, max_bits: int = 200000) -> Optional[str]:
        try:
            from PIL import Image
            img = Image.open(io.BytesIO(data))
            img = img.convert('RGB')
            width, height = img.size
            bits = []
            count = 0
            for y in range(height):
                for x in range(width):
                    r, g, b = img.getpixel((x, y))
                    bits.append(str(b & 1))
                    count += 1
                    if count >= max_bits:
                        break
                if count >= max_bits:
                    break
            msg_bytes = bytearray()
            for i in range(0, (len(bits)//8)*8, 8):
                msg_bytes.append(int(''.join(bits[i:i+8]), 2))
            text = msg_bytes.decode('utf-8', errors='ignore')
            printable = sum(1 for ch in text[:500] if 32 <= ord(ch) < 127)
            if len(text) >= 16 and printable / max(1, len(text[:500])) > 0.75:
                return text.strip('\x00')
        except Exception:
            return None
        return None

    def _add_hidden_finding(self, label: str, content: str, carved_file: Dict[str, Any]):
        try:
            self.results['findings'].append({
                'type': 'hidden_data',
                'display_type': label,
                'icon': 'üïµÔ∏è',
                'data': content[:200],
                'protocol': carved_file.get('source_protocol', 'Unknown'),
                'src_ip': (carved_file.get('source_ips') or ['',''])[0],
                'dst_ip': (carved_file.get('source_ips') or ['',''])[-1],
                'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            })
        except Exception:
            pass

    def _decode_potential_data(self, packet_data_list: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Try lightweight decoding of likely encoded blobs found in packets.

        Returns a list of dicts with keys: 'result', 'original', 'packet_index',
        'protocol', 'confidence', 'chain'. This is intentionally simple and
        non-destructive: failures are swallowed and only safe, text results are
        returned to avoid crashing the pipeline.
        """
        import re, base64, urllib.parse
        results: List[Dict[str, Any]] = []
        if not packet_data_list:
            return results

        b64_re = re.compile(r'([A-Za-z0-9+/]{24,}={0,2})')
        hex_re = re.compile(r'\b([0-9a-fA-F]{32,})\b')

        for p in (packet_data_list or []):
            try:
                text = ''
                if isinstance(p.get('data'), (bytes, bytearray)):
                    try:
                        text = p.get('data').decode('utf-8', errors='ignore')
                    except Exception:
                        text = ''
                else:
                    text = str(p.get('data', ''))

                # Base64 decoding attempts
                for match in b64_re.finditer(text):
                    candidate = match.group(1)
                    try:
                        decoded = base64.b64decode(candidate)
                        # Only return results that are likely text
                        try:
                            decoded_text = decoded.decode('utf-8')
                            # Check if it looks like meaningful text
                            if len(decoded_text) > 4 and any(c.isalpha() for c in decoded_text):
                                results.append({
                                    'result': decoded_text,
                                    'original': candidate,
                                    'packet_index': p.get('packet_index', -1),
                                    'protocol': p.get('protocol', 'Unknown'),
                                    'confidence': 85,
                                    'chain': ['base64']
                                })
                        except UnicodeDecodeError:
                            # Not text, but might be useful binary data
                            pass
                    except Exception:
                        pass

                # Hex decoding attempts
                for match in hex_re.finditer(text):
                    candidate = match.group(1)
                    try:
                        decoded = bytes.fromhex(candidate)
                        # Only return results that are likely text
                        try:
                            decoded_text = decoded.decode('utf-8')
                            # Check if it looks like meaningful text
                            if len(decoded_text) > 4 and any(c.isalpha() for c in decoded_text):
                                results.append({
                                    'result': decoded_text,
                                    'original': candidate,
                                    'packet_index': p.get('packet_index', -1),
                                    'protocol': p.get('protocol', 'Unknown'),
                                    'confidence': 80,
                                    'chain': ['hex']
                                })
                        except UnicodeDecodeError:
                            # Not text, but might be useful binary data
                            pass
                    except Exception:
                        pass

                # URL decoding attempts
                if '%' in text:
                    try:
                        decoded = urllib.parse.unquote(text)
                        if decoded != text:  # Only if decoding changed something
                            results.append({
                                'result': decoded,
                                'original': text,
                                'packet_index': p.get('packet_index', -1),
                                'protocol': p.get('protocol', 'Unknown'),
                                'confidence': 75,
                                'chain': ['url']
                            })
                    except Exception:
                        pass

            except Exception:
                # Silently continue on packet errors to avoid crashing the pipeline
                continue

        return results

    def _extract_http_files(self, packets):
        """Extract files from HTTP transfers"""
        http_files = []
        
        try:
            from scapy.all import TCP, IP, Raw
            
            # Group packets by HTTP sessions
            http_sessions = {}
            for i, pkt in enumerate(packets):
                if not pkt.haslayer(TCP) or not pkt.haslayer(IP):
                    continue
                    
                ip = pkt[IP]
                tcp = pkt[TCP]
                
                # Check for HTTP traffic (ports 80, 443, 8080, 8443)
                if tcp.dport in [80, 443, 8080, 8443] or tcp.sport in [80, 443, 8080, 8443]:
                    session_key = f"{ip.src}:{tcp.sport}-{ip.dst}:{tcp.dport}"
                    if session_key not in http_sessions:
                        http_sessions[session_key] = {
                            'packets': [],
                            'src_ip': ip.src,
                            'dst_ip': ip.dst,
                            'src_port': tcp.sport,
                            'dst_port': tcp.dport
                        }
                    http_sessions[session_key]['packets'].append((i, pkt))
            
            # Analyze each HTTP session
            for session_key, session in http_sessions.items():
                # Reconstruct HTTP stream
                stream_data = b''
                packet_indices = []
                
                for idx, pkt in session['packets']:
                    if pkt.haslayer(Raw):
                        stream_data += pkt[Raw].load
                        packet_indices.append(idx)
                
                if not stream_data:
                    continue
                
                # Look for HTTP responses with file content
                try:
                    # Split by HTTP responses
                    responses = stream_data.split(b'HTTP/1.')
                    for response_data in responses[1:]:  # Skip first part (request)
                        # Look for Content-Type and Content-Length headers
                        content_type_match = re.search(rb'Content-Type:\s*([^\r\n]+)', response_data, re.IGNORECASE)
                        content_length_match = re.search(rb'Content-Length:\s*(\d+)', response_data, re.IGNORECASE)
                        
                        if content_type_match:
                            content_type = content_type_match.group(1).decode('utf-8', errors='ignore').strip()
                            
                            # Look for file content after headers
                            header_end = response_data.find(b'\r\n\r\n')
                            if header_end != -1:
                                file_data = response_data[header_end + 4:]
                                
                                # Limit file size for memory safety
                                if len(file_data) > 50 * 1024 * 1024:  # 50MB limit
                                    file_data = file_data[:50 * 1024 * 1024]
                                
                                if file_data:
                                    # Determine file extension from content type
                                    ext = 'bin'
                                    file_type = 'Binary File'
                                    
                                    if 'image/' in content_type:
                                        ext = content_type.split('/')[-1].split('+')[0]
                                        if ext == 'jpeg':
                                            ext = 'jpg'
                                        file_type = f'{ext.upper()} Image'
                                    elif 'application/pdf' in content_type:
                                        ext = 'pdf'
                                        file_type = 'PDF Document'
                                    elif 'application/zip' in content_type:
                                        ext = 'zip'
                                        file_type = 'ZIP Archive'
                                    elif 'text/' in content_type:
                                        ext = 'txt'
                                        file_type = 'Text File'
                                    
                                    file_hash = hashlib.md5(file_data).hexdigest()
                                    http_file = {
                                        'stream_id': session_key,
                                        'file_type': file_type,
                                        'extension': ext,
                                        'size': len(file_data),
                                        'md5_hash': file_hash,
                                        'position': 0,
                                        'data': file_data,
                                        'filename': f"http_{file_hash[:8]}.{ext}",
                                        'source_protocol': 'HTTP',
                                        'source_ips': [session['src_ip'], session['dst_ip']],
                                        'actual_name': f"downloaded_file.{ext}",
                                        'carving_method': 'http_transfer',
                                        'content_type': content_type
                                    }
                                    http_files.append(http_file)
                
                except Exception as e:
                    # Continue processing other sessions
                    continue
                    
        except Exception as e:
            # Continue with other carving methods
            pass
        
        return http_files

                                'original': text[:200],
                                'packet_index': p.get('packet_index'),
                                'protocol': p.get('protocol'),
                                'confidence': 45,
                                'chain': ['urlencoded']
                            })
                    except Exception:
                        pass

            except Exception:
                # Be conservative: ignore problematic packet entries
                continue

        return results


def analyze_sample_pcap(file_path: str, search_options: Optional[Dict[str, bool]] = None, custom_regex: Optional[str] = None, progress_callback=None, user_decrypt_key: str = None) -> Dict[str, Any]:
    """Convenience wrapper to analyze a single pcap using WebPcapAnalyzer.

    This function exists so other modules (like Streamlit app) can import a
    simple entry-point without instantiating the class directly. It forwards
    arguments to WebPcapAnalyzer.analyze_file and returns the results.
    """
    search_options = search_options or {}
    analyzer = WebPcapAnalyzer()
    return analyzer.analyze_file(file_path, search_options, custom_regex=custom_regex, progress_callback=progress_callback, user_decrypt_key=user_decrypt_key)
